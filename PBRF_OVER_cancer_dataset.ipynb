{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMplkx7rcWqlSYM2uJHZs9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexgill321/EKFAC-Influence-Benchmarks/blob/main/PBRF_OVER_cancer_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekx7SNfzCu4L",
        "outputId": "9c11fbf5-d7f6-417b-8d79-3a532abc61f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x: (569, 30)\n",
            "shape of y: (569,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "x = data['data']\n",
        "y = data['target']\n",
        "print(\"shape of x: {}\\nshape of y: {}\".format(x.shape,y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)"
      ],
      "metadata": {
        "id": "WBFxo3x6Cyct"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32)\n",
        "    self.y = torch.tensor(y,dtype=torch.float32)\n",
        "    self.length = self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "trainset = dataset(X_train,Y_train)\n",
        "trainloader = DataLoader(trainset,batch_size=60,shuffle=False)\n",
        "\n",
        "\n",
        "valset = dataset(X_test,Y_test)\n",
        "valloader = DataLoader(valset,batch_size=60,shuffle=False)"
      ],
      "metadata": {
        "id": "K6dDmDv_C09I"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_shape,32)\n",
        "    self.fc2 = nn.Linear(32,64)\n",
        "    self.fc3 = nn.Linear(64,1)\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "DRTFT14OC2Pz"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "network = Net(x.shape[1])\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "g86hGZTDC3cq"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train(model, train_loader, optimizer, criterion, get_preds_only = False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    if get_preds_only == False:\n",
        "\n",
        "      output_grads = []\n",
        "      for inputs, labels in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels.view(-1, 1))  # Assuming binary classification\n",
        "\n",
        "          outputs.retain_grad()\n",
        "          loss.backward()\n",
        "\n",
        "          output_grads.append(outputs.grad)\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "      return running_loss / len(train_loader), output_grads\n",
        "\n",
        "    else:\n",
        "      model.eval()\n",
        "      all_preds_array = []\n",
        "      for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        all_preds_array.append(outputs)\n",
        "      model.train()\n",
        "      return all_preds_array\n",
        "\n",
        "# Define validation function\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.view(-1, 1))\n",
        "            running_loss += loss.item()\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (predicted == labels.view(-1, 1)).sum().item()\n",
        "            total += labels.size(0)\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
        "\n",
        "\n",
        "training_preds_on_untrained_model = train(network, trainloader, optimizer, criterion, get_preds_only = True)\n",
        "\n",
        "\n",
        "output_grads_global = []\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss, output_grads = train(network, trainloader, optimizer, criterion)\n",
        "    if epoch == 1:\n",
        "      untrained_model_params = network.fc1.weight.grad\n",
        "\n",
        "    output_grads_global = output_grads\n",
        "\n",
        "    val_loss, val_accuracy = validate(network, valloader, criterion)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2%}\")\n",
        "\n",
        "training_preds_on_trained_model = train(network, trainloader, optimizer, criterion, get_preds_only = True)\n",
        "trained_model_params = network.fc1.weight.grad\n",
        "\n",
        "\n",
        "print(torch.equal(untrained_model_params, trained_model_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTEkXp7YC4s-",
        "outputId": "22895825-29a1-45dd-af26-f42e97f5a252"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] - Train Loss: 0.6897, Val Loss: 0.6535, Val Accuracy: 90.21%\n",
            "Epoch [2/20] - Train Loss: 0.6245, Val Loss: 0.5867, Val Accuracy: 90.91%\n",
            "Epoch [3/20] - Train Loss: 0.5485, Val Loss: 0.5036, Val Accuracy: 93.01%\n",
            "Epoch [4/20] - Train Loss: 0.4566, Val Loss: 0.4070, Val Accuracy: 93.01%\n",
            "Epoch [5/20] - Train Loss: 0.3570, Val Loss: 0.3136, Val Accuracy: 93.01%\n",
            "Epoch [6/20] - Train Loss: 0.2663, Val Loss: 0.2413, Val Accuracy: 93.01%\n",
            "Epoch [7/20] - Train Loss: 0.1982, Val Loss: 0.1941, Val Accuracy: 93.01%\n",
            "Epoch [8/20] - Train Loss: 0.1533, Val Loss: 0.1650, Val Accuracy: 92.31%\n",
            "Epoch [9/20] - Train Loss: 0.1253, Val Loss: 0.1463, Val Accuracy: 93.71%\n",
            "Epoch [10/20] - Train Loss: 0.1071, Val Loss: 0.1333, Val Accuracy: 93.01%\n",
            "Epoch [11/20] - Train Loss: 0.0945, Val Loss: 0.1240, Val Accuracy: 93.71%\n",
            "Epoch [12/20] - Train Loss: 0.0853, Val Loss: 0.1172, Val Accuracy: 93.71%\n",
            "Epoch [13/20] - Train Loss: 0.0784, Val Loss: 0.1122, Val Accuracy: 93.71%\n",
            "Epoch [14/20] - Train Loss: 0.0732, Val Loss: 0.1086, Val Accuracy: 94.41%\n",
            "Epoch [15/20] - Train Loss: 0.0692, Val Loss: 0.1057, Val Accuracy: 95.10%\n",
            "Epoch [16/20] - Train Loss: 0.0660, Val Loss: 0.1035, Val Accuracy: 95.10%\n",
            "Epoch [17/20] - Train Loss: 0.0633, Val Loss: 0.1017, Val Accuracy: 95.10%\n",
            "Epoch [18/20] - Train Loss: 0.0610, Val Loss: 0.1002, Val Accuracy: 95.80%\n",
            "Epoch [19/20] - Train Loss: 0.0589, Val Loss: 0.0989, Val Accuracy: 95.80%\n",
            "Epoch [20/20] - Train Loss: 0.0571, Val Loss: 0.0979, Val Accuracy: 95.80%\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((training_preds_on_trained_model[0][4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMEqL3PDV7YO",
        "outputId": "7214354a-acaa-49eb-806e-05959f7baabe"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.1708], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Take Initial PBRF over untrained dataset**"
      ],
      "metadata": {
        "id": "l55J9CKTDZDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bergman_divergance(trainloader, example_prediction, trained_model_example_prediction):\n",
        "\n",
        "  #let's do only for one batch, hence break\n",
        "  for inputs, labels in trainloader:\n",
        "\n",
        "    itr = 0\n",
        "\n",
        "    for individual_inputs_in_batch, individual_label_in_batch in zip(inputs, labels):\n",
        "\n",
        "      loss_on_untrained_examples = criterion(example_prediction[0][itr], individual_label_in_batch.unsqueeze(dim = 0))\n",
        "\n",
        "      loss_on_trained_examples = criterion(trained_model_example_prediction[0][itr], individual_label_in_batch.unsqueeze(dim = 0))\n",
        "\n",
        "      #grad of final prediction wrt loss (not sure):\n",
        "      output_grads_colum_vector = output_grads_global[0][itr].t()\n",
        "\n",
        "      #difference_in_preds_before_vs_after_training\n",
        "      output_difference_vector = example_prediction[0][itr] - trained_model_example_prediction[0][itr]\n",
        "\n",
        "      output_matmul = output_grads_colum_vector * output_difference_vector\n",
        "\n",
        "      final_bergman_for_current_input = loss_on_untrained_examples  - loss_on_trained_examples - output_matmul\n",
        "\n",
        "      print(final_bergman_for_current_input)\n",
        "\n",
        "\n",
        "      itr+=1\n",
        "      break\n",
        "    break\n",
        "\n",
        "\n",
        "  return final_bergman_for_current_input\n",
        "\n",
        "\n",
        "\n",
        "def pbrf_from_bergman(bergman_divergance):\n",
        "\n",
        "  pbrf_for_curr_example = bergman_divergance.item() - torch.sum(torch.square(untrained_model_params - trained_model_params))\n",
        "  return []\n",
        "\n",
        "\n",
        "\n",
        "one_example_bergman_cov = calculate_bergman_divergance(trainloader, training_preds_on_untrained_model, training_preds_on_trained_model)\n",
        "pbrf_from_bergman(one_example_bergman_cov)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJj29Lc-DUOY",
        "outputId": "b0709842-0963-4128-83ab-bdd5e49b1e2b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7209], grad_fn=<SubBackward0>)\n",
            "tensor([[ 3.5539e-02, -2.4995e-03,  3.5185e-02,  3.1117e-02,  1.3292e-02,\n",
            "          1.9750e-02,  1.4400e-02,  2.4263e-02,  1.9477e-02, -2.1316e-02,\n",
            "          1.2991e-02, -8.8752e-02,  1.3538e-02,  1.6757e-02,  6.4057e-03,\n",
            "          1.0079e-02,  1.5489e-03,  2.3873e-02, -1.7038e-02,  6.8573e-04,\n",
            "          3.5793e-02, -7.5127e-03,  3.5941e-02,  3.1242e-02,  1.9469e-02,\n",
            "          1.8692e-02,  1.3061e-02,  3.0947e-02,  2.4895e-02,  8.3003e-04],\n",
            "        [-1.3182e-03, -8.9619e-04, -1.1012e-03, -1.6254e-03,  4.7391e-03,\n",
            "          2.8862e-03,  1.3953e-03,  1.9101e-04,  3.1318e-03,  2.6078e-03,\n",
            "         -3.3650e-03, -1.0149e-02, -2.9982e-03, -2.4240e-03,  5.8581e-03,\n",
            "          3.3209e-03,  2.6222e-03,  2.7097e-03, -1.7009e-03,  3.4707e-03,\n",
            "         -2.3673e-03,  1.0393e-03, -2.2478e-03, -2.7163e-03,  6.4336e-03,\n",
            "          3.0606e-03,  2.5300e-03,  5.7849e-04,  4.3197e-03,  4.5485e-03],\n",
            "        [-1.6142e-02, -4.7555e-03, -1.5125e-02, -1.5518e-02,  5.7431e-03,\n",
            "          3.8063e-03, -1.5084e-03, -7.2083e-03,  1.0220e-02,  1.6975e-02,\n",
            "         -9.5082e-03,  8.9599e-03, -9.2211e-03, -8.7201e-03,  6.2814e-03,\n",
            "          7.4260e-03,  2.9995e-03, -4.7476e-03,  6.9441e-03,  9.3089e-03,\n",
            "         -1.6960e-02, -3.1408e-04, -1.6305e-02, -1.5725e-02,  6.0629e-03,\n",
            "          4.7593e-03,  1.0497e-03, -8.5954e-03,  6.5038e-03,  1.4004e-02],\n",
            "        [ 1.0086e-03,  4.3800e-05,  8.8003e-04,  9.3169e-04, -4.0285e-04,\n",
            "         -9.6802e-04, -1.8545e-04,  6.3939e-06, -7.5236e-04, -1.6527e-03,\n",
            "         -7.9339e-04, -5.2201e-04, -7.3445e-04, -5.1023e-04,  5.6233e-04,\n",
            "         -7.1768e-04, -7.4006e-05,  4.5277e-04,  1.2553e-04, -5.7489e-04,\n",
            "          2.2021e-04, -2.7130e-04,  1.0231e-04,  7.4927e-05, -1.6758e-04,\n",
            "         -9.0675e-04, -2.1483e-04, -7.8420e-05, -3.6481e-04, -1.2228e-03],\n",
            "        [-5.0127e-03, -6.1294e-03, -5.0213e-03, -4.5656e-03, -3.7603e-03,\n",
            "         -7.5313e-03, -7.0469e-03, -5.9137e-03,  1.3834e-03, -2.9907e-03,\n",
            "         -6.0049e-06,  9.7626e-04,  3.4715e-05, -3.9909e-04,  6.7459e-04,\n",
            "         -7.2390e-03, -4.6340e-03, -4.0538e-03,  3.1376e-03, -5.5331e-03,\n",
            "         -4.6891e-03, -9.3179e-03, -4.1163e-03, -3.8389e-03, -6.8359e-03,\n",
            "         -1.7619e-02, -1.5792e-02, -8.8383e-03, -6.4071e-03, -2.0692e-02],\n",
            "        [ 1.7103e-02, -2.0124e-03,  1.7124e-02,  1.4186e-02,  7.7967e-03,\n",
            "          1.2846e-02,  8.9692e-03,  1.2328e-02,  1.5345e-02, -9.4954e-03,\n",
            "          8.2107e-04, -4.3252e-02,  1.7777e-03,  5.0653e-03,  7.0968e-03,\n",
            "          9.8278e-03,  2.9859e-03,  1.3858e-02, -6.2250e-03,  4.3602e-03,\n",
            "          1.4594e-02, -3.3192e-03,  1.4927e-02,  1.2190e-02,  1.2387e-02,\n",
            "          1.2967e-02,  9.2638e-03,  1.5392e-02,  1.6466e-02,  4.5116e-03],\n",
            "        [ 2.1864e-05,  2.0249e-03, -2.2463e-04,  6.6880e-05, -3.1205e-03,\n",
            "         -4.0867e-03, -1.3008e-03, -1.3246e-03, -6.0769e-03, -4.1298e-03,\n",
            "         -1.4613e-03,  6.8400e-03, -1.0249e-03, -1.0369e-03,  1.3401e-03,\n",
            "         -3.2733e-03,  4.0872e-04,  5.3035e-04, -1.6682e-03, -2.5267e-03,\n",
            "         -9.8616e-04,  2.1719e-03, -9.5539e-04, -8.9034e-04, -3.2963e-03,\n",
            "         -6.9503e-03, -3.6749e-03, -2.3129e-03, -6.5644e-03, -8.8652e-03],\n",
            "        [-4.3185e-04, -6.7653e-04, -4.5796e-04, -3.0456e-04, -6.7514e-04,\n",
            "         -8.3354e-04, -7.6625e-04, -6.1653e-04,  2.7430e-04,  8.3550e-05,\n",
            "          2.5184e-04,  1.5414e-03,  1.0849e-04,  1.8547e-05, -1.0335e-03,\n",
            "         -7.6930e-04, -8.7005e-04, -1.0990e-03,  1.2400e-03, -5.8639e-04,\n",
            "         -3.5809e-04, -1.2519e-03, -4.0221e-04, -2.4218e-04, -1.1214e-03,\n",
            "         -8.1876e-04, -9.5309e-04, -8.1169e-04, -2.3193e-04, -5.6556e-04],\n",
            "        [-3.4324e-03, -4.5835e-03, -3.9554e-03, -3.2416e-03, -5.6322e-03,\n",
            "         -1.2520e-02, -7.8741e-03, -7.5290e-03, -4.0050e-03, -8.7194e-03,\n",
            "         -7.1956e-03,  5.3547e-03, -6.5094e-03, -5.6834e-03,  5.7365e-03,\n",
            "         -9.6115e-03, -3.0544e-03, -2.0026e-03,  3.7644e-03, -6.5560e-03,\n",
            "         -7.7953e-03, -7.4141e-03, -7.6976e-03, -7.4079e-03, -7.0557e-03,\n",
            "         -2.2145e-02, -1.6088e-02, -1.1339e-02, -9.7837e-03, -2.5079e-02],\n",
            "        [ 7.6266e-03, -1.1719e-03,  6.8902e-03,  7.2773e-03, -6.1348e-03,\n",
            "         -7.6080e-03, -3.3899e-03,  6.0766e-04, -4.8385e-03, -1.3864e-02,\n",
            "          3.3079e-03, -4.8429e-03,  3.3849e-03,  3.5036e-03, -1.8296e-03,\n",
            "         -8.8101e-03, -4.3534e-03,  1.2125e-03, -1.2964e-03, -8.8570e-03,\n",
            "          6.6625e-03, -6.0068e-03,  6.5305e-03,  6.2502e-03, -7.6325e-03,\n",
            "         -1.4198e-02, -1.0180e-02, -5.8698e-04, -7.4180e-03, -2.1898e-02],\n",
            "        [ 1.5709e-02, -2.0108e-03,  1.5744e-02,  1.3062e-02,  8.1738e-03,\n",
            "          1.2058e-02,  8.3016e-03,  1.1457e-02,  1.4025e-02, -8.4153e-03,\n",
            "          1.0813e-03, -4.2973e-02,  1.9182e-03,  4.7868e-03,  7.2201e-03,\n",
            "          8.9927e-03,  2.9285e-03,  1.3128e-02, -6.5844e-03,  4.0941e-03,\n",
            "          1.3646e-02, -3.0307e-03,  1.3933e-02,  1.1353e-02,  1.2540e-02,\n",
            "          1.2075e-02,  8.6439e-03,  1.4450e-02,  1.5625e-02,  4.4815e-03],\n",
            "        [-8.3695e-03,  5.0101e-03, -8.2299e-03, -7.3933e-03, -6.5468e-03,\n",
            "         -2.8535e-03, -1.0037e-03, -4.5330e-03, -7.0142e-03,  6.4743e-03,\n",
            "         -3.4224e-03,  3.9118e-02, -3.3573e-03, -3.8696e-03, -5.4745e-03,\n",
            "          4.2636e-04,  1.1474e-03, -6.4589e-03,  5.2271e-03,  1.6141e-03,\n",
            "         -8.4112e-03,  7.5449e-03, -8.4243e-03, -6.9982e-03, -7.9107e-03,\n",
            "          9.3889e-04,  1.9319e-03, -5.8790e-03, -7.9779e-03,  5.7530e-03],\n",
            "        [ 1.4444e-03,  2.0841e-03,  1.4799e-03,  9.8856e-04, -2.9639e-04,\n",
            "          1.8388e-03,  2.0232e-03,  1.5430e-03, -2.9928e-04, -1.0764e-03,\n",
            "         -1.3539e-03,  2.2131e-03, -8.6936e-04, -2.4636e-04,  1.3178e-03,\n",
            "          2.2510e-03,  1.9239e-03,  2.2654e-03, -1.4842e-03,  1.4069e-03,\n",
            "          6.8244e-04,  3.1698e-03,  8.4276e-04,  5.1800e-04,  6.2776e-04,\n",
            "          1.9870e-03,  2.3452e-03,  1.7181e-03, -1.9914e-04,  7.8250e-04],\n",
            "        [ 2.5408e-04, -5.9169e-04,  1.8020e-04,  2.6813e-04, -8.2273e-04,\n",
            "         -1.3695e-03, -8.9986e-04, -4.7158e-04, -4.2124e-04, -1.4849e-03,\n",
            "          2.0038e-04, -3.6710e-04,  2.0800e-04,  1.8469e-04, -3.5901e-05,\n",
            "         -1.4205e-03, -7.4459e-04, -2.2365e-04,  1.5267e-04, -1.2416e-03,\n",
            "          1.4074e-04, -1.2653e-03,  1.6005e-04,  1.6568e-04, -1.1799e-03,\n",
            "         -2.7075e-03, -2.1508e-03, -8.1557e-04, -1.1895e-03, -3.5641e-03],\n",
            "        [-8.9343e-03, -4.6581e-03, -9.1789e-03, -7.7739e-03, -1.0158e-02,\n",
            "         -1.1842e-02, -8.9128e-03, -9.7967e-03, -1.4625e-03, -2.5559e-04,\n",
            "         -4.5637e-03,  3.1681e-02, -4.8957e-03, -4.9095e-03, -5.6245e-03,\n",
            "         -8.1182e-03, -6.1324e-03, -1.0721e-02,  1.2811e-02, -5.2547e-03,\n",
            "         -1.0951e-02, -8.3117e-03, -1.0987e-02, -9.2830e-03, -1.4239e-02,\n",
            "         -1.5426e-02, -1.3276e-02, -1.3772e-02, -9.3370e-03, -1.3383e-02],\n",
            "        [-1.4147e-03, -4.6223e-04, -1.2982e-03, -1.4314e-03,  5.5298e-04,\n",
            "          7.4515e-04,  9.2798e-05, -5.1464e-04,  1.5231e-03,  1.6337e-03,\n",
            "         -1.1543e-03,  1.0029e-03, -1.0748e-03, -8.9125e-04,  7.0143e-04,\n",
            "          1.1674e-03,  4.2450e-04, -2.6638e-04,  8.1407e-04,  1.1888e-03,\n",
            "         -1.6247e-03,  8.8959e-06, -1.5293e-03, -1.5179e-03,  6.7570e-04,\n",
            "          8.8484e-04,  3.8130e-04, -6.3979e-04,  9.7027e-04,  1.6573e-03],\n",
            "        [ 1.3202e-02,  3.2722e-03,  1.2193e-02,  1.2653e-02, -4.8878e-03,\n",
            "         -5.2249e-03,  3.4827e-04,  4.7186e-03, -9.0819e-03, -1.5477e-02,\n",
            "          4.7076e-03, -7.4051e-03,  4.6186e-03,  4.7178e-03, -2.9701e-03,\n",
            "         -7.1261e-03, -2.3452e-03,  4.1289e-03, -4.4748e-03, -7.9333e-03,\n",
            "          1.1965e-02, -5.4903e-04,  1.1222e-02,  1.0797e-02, -4.6337e-03,\n",
            "         -5.8499e-03, -1.5643e-03,  5.4225e-03, -5.5748e-03, -1.2814e-02],\n",
            "        [-2.1631e-03,  4.6470e-03, -1.6673e-03, -2.2173e-03,  5.1715e-03,\n",
            "          8.8644e-03,  6.0361e-03,  2.9264e-03,  1.6312e-03,  9.9786e-03,\n",
            "         -1.6196e-03,  4.4725e-03, -1.5843e-03, -1.4928e-03,  4.8058e-04,\n",
            "          9.3811e-03,  5.3840e-03,  1.5442e-03, -1.5003e-03,  8.3049e-03,\n",
            "         -1.4100e-03,  9.5452e-03, -1.5107e-03, -1.5172e-03,  7.5842e-03,\n",
            "          1.7592e-02,  1.4304e-02,  5.1210e-03,  6.8500e-03,  2.3324e-02],\n",
            "        [ 6.7786e-03,  5.8161e-03,  7.6566e-03,  5.0278e-03,  1.6213e-02,\n",
            "          2.3234e-02,  1.5553e-02,  1.2681e-02,  1.2097e-02,  1.1782e-02,\n",
            "         -1.1496e-03, -2.8217e-02, -6.7518e-04,  1.0290e-03,  6.8803e-03,\n",
            "          2.1395e-02,  1.1092e-02,  1.1919e-02, -8.4617e-03,  1.6649e-02,\n",
            "          7.2266e-03,  1.3536e-02,  7.1954e-03,  5.3566e-03,  2.3367e-02,\n",
            "          3.7650e-02,  2.9724e-02,  1.8703e-02,  2.3027e-02,  4.2633e-02],\n",
            "        [-2.4222e-03, -9.8088e-04, -2.3374e-03, -2.3678e-03,  8.1119e-04,\n",
            "         -1.9503e-04, -5.2271e-04, -1.5461e-03,  1.4137e-03,  1.9250e-03,\n",
            "         -2.8654e-03,  1.3822e-03, -2.7425e-03, -2.4024e-03,  1.9333e-03,\n",
            "          8.4239e-04,  5.6297e-04, -5.5254e-04,  1.6240e-03,  1.3964e-03,\n",
            "         -3.4166e-03, -3.5749e-04, -3.3979e-03, -3.3015e-03,  1.1215e-03,\n",
            "          6.4295e-05, -1.1763e-05, -1.9346e-03,  1.0117e-03,  1.7180e-03],\n",
            "        [ 1.8014e-02,  8.7339e-04,  1.7368e-02,  1.5667e-02,  4.1188e-03,\n",
            "          4.5569e-03,  6.1763e-03,  9.8477e-03,  3.4426e-03, -1.5494e-02,\n",
            "         -1.0803e-03, -3.5658e-02, -1.0941e-04,  2.6501e-03,  7.9684e-03,\n",
            "          2.6897e-03,  2.6130e-03,  1.3320e-02, -7.6413e-03, -4.7942e-04,\n",
            "          1.3578e-02, -8.3452e-04,  1.3235e-02,  1.1037e-02,  8.6646e-03,\n",
            "          4.4103e-03,  6.0251e-03,  1.2074e-02,  7.8285e-03, -3.9591e-03],\n",
            "        [ 7.6842e-03,  1.9174e-03,  7.0116e-03,  7.4527e-03, -4.5479e-03,\n",
            "         -4.3810e-03, -4.9613e-04,  2.1903e-03, -6.0184e-03, -9.8281e-03,\n",
            "          2.7239e-03,  1.0840e-04,  2.6087e-03,  2.6794e-03, -3.1561e-03,\n",
            "         -5.1872e-03, -2.1699e-03,  1.3385e-03, -1.2550e-03, -5.5413e-03,\n",
            "          6.7448e-03, -9.2410e-04,  6.2513e-03,  6.1979e-03, -4.8564e-03,\n",
            "         -4.6878e-03, -1.8704e-03,  2.2858e-03, -4.5613e-03, -8.8782e-03],\n",
            "        [ 1.9283e-02, -3.4805e-03,  1.9336e-02,  1.6153e-02,  1.1892e-02,\n",
            "          1.4931e-02,  9.8526e-03,  1.4094e-02,  1.7645e-02, -9.7764e-03,\n",
            "          2.3076e-03, -5.9668e-02,  3.1345e-03,  6.3083e-03,  9.6864e-03,\n",
            "          1.0497e-02,  3.3083e-03,  1.6348e-02, -9.0756e-03,  4.8032e-03,\n",
            "          1.7301e-02, -4.8654e-03,  1.7583e-02,  1.4346e-02,  1.7296e-02,\n",
            "          1.4756e-02,  1.0276e-02,  1.8065e-02,  2.0472e-02,  5.8612e-03],\n",
            "        [-5.9493e-03, -1.1772e-03, -5.4372e-03, -5.6447e-03,  2.2744e-03,\n",
            "          3.0043e-03,  9.7072e-05, -1.7027e-03,  4.0214e-03,  7.5695e-03,\n",
            "         -6.9941e-04,  3.2468e-03, -7.5042e-04, -1.0817e-03,  3.7565e-04,\n",
            "          3.3729e-03,  9.4122e-04, -2.0379e-03,  1.4019e-03,  3.5199e-03,\n",
            "         -4.5390e-03,  5.9250e-04, -4.1445e-03, -3.9499e-03,  1.9084e-03,\n",
            "          3.2350e-03,  8.8925e-04, -1.8324e-03,  2.4148e-03,  6.1791e-03],\n",
            "        [ 1.3364e-02,  2.2558e-03,  1.3093e-02,  1.1263e-02,  4.1217e-03,\n",
            "          6.5836e-03,  6.8072e-03,  8.7380e-03,  3.6498e-03, -1.0164e-02,\n",
            "         -1.5437e-03, -2.6014e-02, -4.2035e-04,  1.9944e-03,  7.3920e-03,\n",
            "          5.2934e-03,  4.1183e-03,  1.1838e-02, -7.4680e-03,  2.0675e-03,\n",
            "          1.0158e-02,  2.5930e-03,  1.0187e-02,  8.2197e-03,  8.3265e-03,\n",
            "          6.6021e-03,  7.2285e-03,  1.0738e-02,  6.8413e-03, -5.7410e-05],\n",
            "        [ 8.2766e-03,  4.0470e-04,  7.9125e-03,  7.2641e-03, -2.1308e-03,\n",
            "          6.6273e-04,  1.9560e-03,  3.9035e-03,  1.8448e-03, -8.0888e-03,\n",
            "          5.7956e-05, -5.0365e-03,  3.9336e-04,  1.8178e-03, -5.5422e-04,\n",
            "          3.3105e-04, -5.8507e-04,  3.6300e-03,  5.3799e-06, -1.5056e-03,\n",
            "          6.1901e-03, -1.7367e-03,  6.0592e-03,  5.4218e-03, -1.2128e-03,\n",
            "          8.1003e-04,  1.2940e-03,  4.3152e-03,  1.5674e-03, -3.7071e-03],\n",
            "        [ 7.2875e-03,  1.0233e-03,  7.7848e-03,  5.6813e-03,  1.1653e-02,\n",
            "          1.5439e-02,  9.8771e-03,  9.2608e-03,  1.1939e-02,  4.5457e-03,\n",
            "         -7.3490e-04, -2.9664e-02, -3.5930e-04,  1.3822e-03,  6.1722e-03,\n",
            "          1.3658e-02,  6.1513e-03,  9.4686e-03, -5.3197e-03,  1.0053e-02,\n",
            "          6.7877e-03,  4.4624e-03,  6.8239e-03,  5.1592e-03,  1.6568e-02,\n",
            "          2.2949e-02,  1.7395e-02,  1.3183e-02,  1.7860e-02,  2.4082e-02],\n",
            "        [ 1.5373e-02, -2.1468e-04,  1.5329e-02,  1.2845e-02,  8.4775e-03,\n",
            "          1.1056e-02,  8.4465e-03,  1.1189e-02,  1.0105e-02, -8.8915e-03,\n",
            "          3.6021e-04, -4.2350e-02,  1.3580e-03,  3.9745e-03,  8.9365e-03,\n",
            "          8.2004e-03,  4.2084e-03,  1.4062e-02, -8.8935e-03,  3.8841e-03,\n",
            "          1.3096e-02, -1.5345e-04,  1.3297e-02,  1.0719e-02,  1.3327e-02,\n",
            "          1.0947e-02,  9.0320e-03,  1.4206e-02,  1.3361e-02,  3.6288e-03],\n",
            "        [ 1.3883e-03, -6.3224e-04,  1.1295e-03,  1.2836e-03, -1.1974e-03,\n",
            "         -2.8102e-03, -1.1789e-03, -7.1066e-04, -1.3136e-03, -3.6051e-03,\n",
            "         -1.7478e-03, -8.9108e-04, -1.6176e-03, -1.1934e-03,  1.3082e-03,\n",
            "         -2.2205e-03, -6.3585e-04,  4.4892e-04,  6.7655e-04, -1.7161e-03,\n",
            "         -2.2665e-04, -1.5966e-03, -4.0258e-04, -4.2327e-04, -1.0434e-03,\n",
            "         -3.7863e-03, -2.1798e-03, -1.2238e-03, -1.4027e-03, -4.6923e-03],\n",
            "        [-1.0483e-03,  7.3595e-04, -1.2794e-03, -8.8444e-04, -4.0802e-03,\n",
            "         -5.6433e-03, -2.8629e-03, -2.5015e-03, -5.8511e-03, -4.7663e-03,\n",
            "         -8.0315e-04,  7.0714e-03, -3.8742e-04, -6.1808e-04,  1.0531e-03,\n",
            "         -4.9856e-03, -7.6321e-04, -4.6075e-04, -1.2380e-03, -3.9511e-03,\n",
            "         -1.6196e-03,  1.0697e-04, -1.4235e-03, -1.2999e-03, -5.1048e-03,\n",
            "         -1.1006e-02, -7.4271e-03, -4.1136e-03, -8.2405e-03, -1.3863e-02],\n",
            "        [-1.4594e-03, -1.6586e-03, -1.4531e-03, -1.1249e-03,  9.3716e-04,\n",
            "         -1.0765e-03, -1.4016e-03, -1.2416e-03,  4.4309e-04,  1.3124e-03,\n",
            "          5.8537e-04, -3.0313e-03,  2.6168e-04, -1.8552e-04, -1.3219e-04,\n",
            "         -1.3344e-03, -1.0621e-03, -1.3974e-03,  7.7952e-04, -6.0469e-04,\n",
            "         -9.6658e-04, -2.1455e-03, -1.0732e-03, -8.7677e-04,  4.5463e-04,\n",
            "         -1.1998e-03, -1.4910e-03, -1.3282e-03,  6.0317e-04,  1.2352e-05],\n",
            "        [ 8.2914e-03, -2.4708e-04,  8.3295e-03,  6.8257e-03,  4.3726e-03,\n",
            "          6.7756e-03,  4.9076e-03,  6.3028e-03,  6.7542e-03, -4.4122e-03,\n",
            "          1.5210e-04, -2.1759e-02,  7.3372e-04,  2.3193e-03,  4.3868e-03,\n",
            "          5.2949e-03,  2.2804e-03,  7.5261e-03, -4.2150e-03,  2.6108e-03,\n",
            "          7.0892e-03, -2.3919e-04,  7.2851e-03,  5.8583e-03,  6.9516e-03,\n",
            "          6.8062e-03,  5.2414e-03,  7.9269e-03,  7.8453e-03,  2.6842e-03]])\n",
            "tensor(0.6322)\n",
            "******\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have the true and predicted values\n",
        "Y = torch.tensor([1.0, 2.0, 3.0])  # Example true values\n",
        "Y_hat = torch.tensor([1.2, 1.8, 2.9], requires_grad=True)  # Example predicted values\n",
        "\n",
        "# Assuming a simple loss function (mean squared error)\n",
        "loss_function = torch.nn.MSELoss()\n",
        "\n",
        "# Calculating the loss\n",
        "loss = loss_function(Y_hat, Y)\n",
        "print(Y_hat.grad)\n",
        "loss.backward()  # Compute gradients\n",
        "\n",
        "# Get the gradient with respect to predicted values\n",
        "gradient_wrt_Yhat = Y_hat.grad\n",
        "print(gradient_wrt_Yhat)\n",
        "\n",
        "# Transpose the gradient (reshaping it to a column vector)\n",
        "gradient_transposed = gradient_wrt_Yhat.view(-1, 1)\n",
        "\n",
        "print(\"Gradient with respect to Y_hat:\\n\", gradient_wrt_Yhat)\n",
        "print(\"Transposed Gradient:\\n\", gradient_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuxMY46p5m_",
        "outputId": "bcc7e5e8-23a9-428f-a8a4-825633232e86"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([ 0.1333, -0.1333, -0.0667])\n",
            "Gradient with respect to Y_hat:\n",
            " tensor([ 0.1333, -0.1333, -0.0667])\n",
            "Transposed Gradient:\n",
            " tensor([[ 0.1333],\n",
            "        [-0.1333],\n",
            "        [-0.0667]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQOctzDTp6dO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}