{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWuSel7j0q30",
        "outputId": "9085daa0-34f2-4b16-a63d-4e1318dac322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of x: (569, 30)\n",
            "shape of y: (569,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "x = data['data']\n",
        "y = data['target']\n",
        "print(\"shape of x: {}\\nshape of y: {}\".format(x.shape,y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "TL_ucBKL0wp8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "FGiJ0k8v00qi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32)\n",
        "    self.y = torch.tensor(y,dtype=torch.float32)\n",
        "    self.length = self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "trainset = dataset(x,y)\n",
        "#DataLoader\n",
        "trainloader = DataLoader(trainset,batch_size=60,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "wuaaUbt97Ghg"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_shape,32)\n",
        "    self.fc2 = nn.Linear(32,64)\n",
        "    self.fc3 = nn.Linear(64,1)\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "kibxNxZR89YB"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "network = Net(x.shape[1])\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "AKKZs6Py9Q3-"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "class EKFACDistilled(Optimizer):\n",
        "    def __init__(self, net, eps):\n",
        "        self.eps = eps\n",
        "        self.params = []\n",
        "        self._fwd_handles = []\n",
        "        self._bwd_handles = []\n",
        "        self.net = net\n",
        "        for mod in net.modules():\n",
        "          mod_class = mod.__class__.__name__\n",
        "          if mod_class in ['Linear']:\n",
        "              handle = mod.register_forward_pre_hook(self._save_input)\n",
        "              self._fwd_handles.append(handle)\n",
        "              handle = mod.register_full_backward_hook(self._save_grad_output)\n",
        "              self._bwd_handles.append(handle)\n",
        "              params = [mod.weight]\n",
        "              if mod.bias is not None:\n",
        "                  params.append(mod.bias)\n",
        "              d = {'params': params, 'mod': mod, 'layer_type': mod_class}\n",
        "              self.params.append(d)\n",
        "        super(EKFACDistilled, self).__init__(self.params, {})\n",
        "\n",
        "    def step(self, update_stats=True, update_params=True):\n",
        "        for group in self.param_groups:\n",
        "            if len(group['params']) == 2:\n",
        "                weight, bias = group['params']\n",
        "            else:\n",
        "                weight = group['params'][0]\n",
        "                bias = None\n",
        "            state = self.state[weight]\n",
        "\n",
        "            self._compute_kfe(group, state)\n",
        "\n",
        "            self._precond(weight, bias, group, state)\n",
        "\n",
        "    def _compute_kfe(self, group, state):\n",
        "        mod = group['mod']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        print(f\"Shape of x: {x.shape}\")\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        print(f\"Shape of gy: {gy.shape}\")\n",
        "\n",
        "        # Computation of xxt\n",
        "        x = x.data.t() # transpose of activations\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if mod.bias is not None:\n",
        "            ones = torch.ones_like(x[:1])\n",
        "            x = torch.cat([x, ones], dim=0)\n",
        "\n",
        "        # Calculate covariance matrix for activations (A_{l-1})\n",
        "        xxt = torch.mm(x, x.t()) / float(x.shape[1])\n",
        "\n",
        "        print(f'A cov matrix shape: {xxt.shape}')\n",
        "\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaA, QA)\n",
        "        la, Qa = torch.linalg.eigh(xxt, UPLO='U')\n",
        "        state['Qa'] = Qa\n",
        "        print(f'Qa eigenvec shape: {Qa.shape}')\n",
        "        print(f'LambdaA eigenval vec shape: {la.shape}')\n",
        "        # Computation of ggt\n",
        "        gy = gy.data.t()\n",
        "\n",
        "        # Calculate covariance matrix for layer outputs (S_{l})\n",
        "        ggt = torch.mm(gy, gy.t()) / float(gy.shape[1])\n",
        "\n",
        "        print(f'S cov matrix shape: {ggt.shape}')\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaS, QS)\n",
        "        ls, Qs = torch.linalg.eigh(ggt, UPLO='U')\n",
        "\n",
        "        G_real = torch.kron(xxt,ggt)\n",
        "        print(f'AxS direct shape: {G_real.shape}')\n",
        "\n",
        "        state['Qs'] = Qs\n",
        "\n",
        "        print(f'Qs eigenvec shape: {Qs.shape}')\n",
        "        print(f'LambdaS eigenval vec shape: {ls.shape}')\n",
        "\n",
        "        prod_as = torch.kron(Qa, Qs)\n",
        "\n",
        "        print(f'Kroneker product of Qa * Qs: {prod_as.shape}')\n",
        "\n",
        "        prod_eigval = torch.kron(torch.diag(la),torch.diag(ls))\n",
        "\n",
        "        G = torch.matmul(prod_as,torch.matmul(prod_eigval, prod_as.t()))\n",
        "\n",
        "        print(f'G SHAPE: {G.shape}')\n",
        "\n",
        "        print(f'Kroneker product of LambdaA * LambdaS: {prod_eigval.shape}')\n",
        "\n",
        "        # Outer product of the eigenvalue vectors. Of shape (len(s) x len(a))\n",
        "        state[\"m2\"] = m2 = ls.unsqueeze(1) * la.unsqueeze(0)\n",
        "        print(f\"eigenval outer product shape: {m2.shape}\")\n",
        "\n",
        "        print(G_real - G)\n",
        "\n",
        "    def _precond(self, weight, bias, group, state):\n",
        "        \"\"\"Applies preconditioning.\"\"\"\n",
        "        Qa = state['Qa']\n",
        "        Qs = state['Qs']\n",
        "        m2 = state['m2']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        print(x)\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        g = weight.grad.data\n",
        "        s = g.shape\n",
        "        s_x = x.size()\n",
        "        s_gy = gy.size()\n",
        "        bs = x.size(0)\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if bias is not None:\n",
        "            ones = torch.ones_like(x[:,:1])\n",
        "            x = torch.cat([x, ones], dim=1)\n",
        "\n",
        "        # KFE of activations ??\n",
        "        x_kfe = torch.mm(x, Qa)\n",
        "\n",
        "        print(f\"KFE of activations a shape: {x_kfe.shape}\")\n",
        "\n",
        "        # KFE of layer outputs ??\n",
        "        gy_kfe = torch.mm(gy, Qs)\n",
        "\n",
        "        print(f\"KFE of outputs gy shape: {gy_kfe.shape}\")\n",
        "\n",
        "        m2 = torch.mm(gy_kfe.t()**2, x_kfe**2) / bs\n",
        "\n",
        "        print(f'kfe squared matrix idk shape: {m2.shape}')\n",
        "        g_kfe = torch.mm(gy_kfe.t(), x_kfe) / bs\n",
        "\n",
        "        print(f'g_kfe shape: {g_kfe.shape}')\n",
        "\n",
        "        g_nat_kfe = g_kfe / (m2 + self.eps)\n",
        "\n",
        "        print(f'g_nat_kfe shape: {g_nat_kfe.shape}')\n",
        "\n",
        "        g_nat = torch.mm(g_nat_kfe, Qs.t())\n",
        "\n",
        "        if bias is not None:\n",
        "            gb = g_nat[:, -1].contiguous().view(*bias.shape)\n",
        "            bias.grad.data = gb\n",
        "            g_nat = g_nat[:, :-1]\n",
        "\n",
        "        g_nat = g_nat.contiguous().view(*s)\n",
        "        weight.grad.data = g_nat\n",
        "\n",
        "    def _save_input(self, mod, i):\n",
        "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['x'] = i[0]\n",
        "\n",
        "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
        "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcuk4RoJ9dIM",
        "outputId": "6096bb71-01cb-4934-e929-35ed660117e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n"
          ]
        }
      ],
      "source": [
        "precond = EKFACDistilled(network, eps=0.001)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "for mod in network.modules():\n",
        "  mod_class = mod.__class__.__name__\n",
        "  print(mod_class)\n",
        "  print(\"**********************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "TAjBDMe3nm6I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YmB6iG_9dyA",
        "outputId": "7e70ef48-d35e-4646-8792-77ffcb4dbdcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[ 1.0971, -2.0733,  1.2699,  ...,  2.2961,  2.7506,  1.9370],\n",
            "        [ 1.8298, -0.3536,  1.6860,  ...,  1.0871, -0.2439,  0.2812],\n",
            "        [ 1.5799,  0.4562,  1.5665,  ...,  1.9550,  1.1523,  0.2014],\n",
            "        ...,\n",
            "        [ 0.1655,  0.5353,  0.1475,  ...,  1.0475,  1.2898,  1.4106],\n",
            "        [-0.3060,  0.0047, -0.3855,  ..., -1.5759, -0.7470, -1.1668],\n",
            "        [-1.5647, -1.7452, -1.5499,  ..., -1.0722,  0.5165,  0.3499]])\n",
            "Target Shape: torch.Size([60])\n",
            "torch.Size([60, 1])\n",
            "torch.Size([32, 30])\n"
          ]
        }
      ],
      "source": [
        "for i, (inputs, targets) in enumerate(trainloader):\n",
        "  optimizer.zero_grad\n",
        "  print(f'Input Shape: {inputs.shape}')\n",
        "  print(inputs)\n",
        "  print(f'Target Shape: {targets.shape}')\n",
        "  outputs = network(inputs)\n",
        "  print(outputs.shape)\n",
        "  outputs[0].backward()\n",
        "  #print(f'Output Shape: {targets.shape}')\n",
        "  #loss = criterion(outputs, targets.reshape(-1,1))\n",
        "  #loss.backward()\n",
        "\n",
        "  print(network.fc1.weight.grad.shape)\n",
        "  #precond.step()\n",
        "\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
