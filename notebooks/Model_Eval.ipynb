{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.linear_nn import get_model, load_model, test, load_data\n",
    "from src.model_eval import train_dataset\n",
    "from src.model_eval import train_loader\n",
    "from torch.cpu.amp import autocast\n",
    "net, criterion, optimizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(net)\n",
    "model = load_model(net, filepath='../models/linear_trained_model.pth')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class EKFACDistilled(Optimizer):\n",
    "    def __init__(self, net, eps):\n",
    "        self.eps = eps\n",
    "        self.params = []\n",
    "        self._fwd_handles = []\n",
    "        self._bwd_handles = []\n",
    "        self.net = net\n",
    "        self.calc_act = True\n",
    "\n",
    "        for mod in net.modules():\n",
    "            mod_class = mod.__class__.__name__\n",
    "            if mod_class in ['Linear']:\n",
    "                handle = mod.register_forward_pre_hook(self._save_input)\n",
    "                self._fwd_handles.append(handle)\n",
    "                handle = mod.register_full_backward_hook(self._save_grad_output)\n",
    "                self._bwd_handles.append(handle)\n",
    "                params = [mod.weight]\n",
    "                if mod.bias is not None:\n",
    "                    params.append(mod.bias)\n",
    "                d = {'params': params, 'mod': mod, 'layer_type': mod_class, 'A': [], 'S': []}\n",
    "                self.params.append(d)\n",
    "        super(EKFACDistilled, self).__init__(self.params, {})\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            mod = group['mod']\n",
    "            x = self.state[mod]['x']\n",
    "            gy = self.state[mod]['gy']\n",
    "\n",
    "            # Computation of activation cov matrix for batch\n",
    "            x = x.data.t()\n",
    "\n",
    "            # Append column of ones to x if bias is not None\n",
    "            if mod.bias is not None:\n",
    "                ones = torch.ones_like(x[:1])\n",
    "                x = torch.cat([x, ones], dim=0)\n",
    "            \n",
    "            if self.calc_act:\n",
    "                # Calculate covariance matrix for activations (A_{l-1})\n",
    "                group['A'].append(torch.mm(x, x.t()) / float(x.shape[1]))\n",
    "\n",
    "            # Computation of psuedograd of layer output cov matrix for batch\n",
    "            gy = gy.data.t()\n",
    "\n",
    "            # Calculate covariance matrix for layer outputs (S_{l})\n",
    "            group['S'].append(torch.mm(gy, gy.t()) / float(gy.shape[1]))\n",
    "\n",
    "    def _save_input(self, mod, i):\n",
    "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
    "        self.state[mod]['x'] = i[0]\n",
    "\n",
    "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
    "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
    "        self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum._utils.common as common\n",
    "from captum.influence._core.influence import DataInfluence\n",
    "from torch.nn import Module\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch import Tensor\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class EKFACInfluence(DataInfluence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        module: Module,\n",
    "        layers: Union[str, List[str]],\n",
    "        influence_src_dataset: Dataset,\n",
    "        activation_dir: str,\n",
    "        model_id: str = \"\",\n",
    "        batch_size: int = 1,\n",
    "        query_batch_size: int = 1,\n",
    "        cov_batch_size: int = 1,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            module (Module): An instance of pytorch model. This model should define all of its\n",
    "                layers as attributes of the model. The output of the model must be logits for the\n",
    "                classification task.\n",
    "            layers (Union[str, List[str]]): A list of layer names for which the influence will\n",
    "                be computed.\n",
    "            influence_src_dataset (torch.utils.data.Dataset): Pytorch dataset that is used to create\n",
    "                a pytorch dataloader to iterate over the dataset. This is the dataset for which we will\n",
    "                be seeking for influential instances. In most cases this is the training dataset.\n",
    "            activation_dir (str): Path to the directory where the activation computations will be stored.\n",
    "            model_id (str): The name/version of the model for which layer activations are being computed.\n",
    "                Activations will be stored and loaded under the subdirectory with this name if provided.\n",
    "            batch_size (int): Batch size for the dataloader used to iterate over the influence_src_dataset.\n",
    "            **kwargs: Any additional arguments that are necessary for specific implementations of the\n",
    "                'DataInfluence' abstract class.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.module = module\n",
    "        self.module.to(self.device)\n",
    "        self.layers = [layers] if isinstance(layers, str) else layers\n",
    "        self.influence_src_dataset = influence_src_dataset\n",
    "        self.activation_dir = activation_dir\n",
    "        self.model_id = model_id\n",
    "        self.query_batch_size = query_batch_size\n",
    "\n",
    "        self.influence_src_dataloader = DataLoader(\n",
    "            self.influence_src_dataset, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        self.cov_src_dataloader = DataLoader(\n",
    "            self.influence_src_dataset, batch_size=cov_batch_size, shuffle=True\n",
    "        )\n",
    "            \n",
    "    def influence(\n",
    "            self,\n",
    "            query_dataset: Dataset,\n",
    "            topk: int = 1,\n",
    "            eps: float = 1e-5,\n",
    "            **kwargs: Any,\n",
    "        ) -> Dict:\n",
    "\n",
    "        influences: Dict[str, Any] = {}\n",
    "        query_grads: Dict[str, List[Tensor]] = {}\n",
    "        influence_src_grads: Dict[str, List[Tensor]] = {}\n",
    "\n",
    "        query_dataloader = DataLoader(\n",
    "            query_dataset, batch_size=self.query_batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        layer_modules = [\n",
    "            common._get_module_from_name(self.module, layer) for layer in self.layers\n",
    "        ]\n",
    "\n",
    "        G_list = self._compute_EKFAC_params()\n",
    "\n",
    "        criterion = torch.nn.NLLLoss(reduction='sum')\n",
    "        print(f'Cacultating query gradients on trained model')\n",
    "        for layer in layer_modules:\n",
    "            query_grads[layer] = []\n",
    "            influence_src_grads[layer] = []\n",
    "\n",
    "        for _, (inputs, targets) in tqdm.tqdm(enumerate(query_dataloader), total=len(query_dataloader)):\n",
    "            self.module.zero_grad()\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            outputs = self.module(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            for layer in layer_modules:\n",
    "                Qa = G_list[layer]['Qa']\n",
    "                Qs = G_list[layer]['Qs']\n",
    "                eigenval_diag = G_list[layer]['lambda']\n",
    "                if layer.bias is not None:\n",
    "                    grad_bias = layer.bias.grad\n",
    "                    grad_weights = layer.weight.grad\n",
    "                    grad_bias = grad_bias.reshape(-1, 1)\n",
    "                    grads = torch.cat((grad_weights, grad_bias), dim=1)\n",
    "                else:\n",
    "                    grads = layer.weight.grad\n",
    "\n",
    "                p1 = torch.matmul(Qs, torch.matmul(grads, torch.t(Qa)))\n",
    "                p2 = torch.reciprocal(eigenval_diag+eps).reshape(p1.shape[0], -1)\n",
    "                ihvp = torch.flatten(torch.matmul(torch.t(Qs), torch.matmul((p1/p2), Qa)))\n",
    "                query_grads[layer].append(ihvp)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        print(f'Cacultating training src gradients on trained model')\n",
    "        for i, (inputs, targets) in tqdm.tqdm(enumerate(self.influence_src_dataloader), total=len(self.influence_src_dataloader)):\n",
    "            self.module.zero_grad()\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            outputs = self.module(inputs)\n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            for single_loss in loss:\n",
    "                single_loss.backward(retain_graph=True)\n",
    "\n",
    "                for layer in layer_modules:\n",
    "                    if layer.bias is not None:\n",
    "                        grad_bias = layer.bias.grad\n",
    "                        grad_weights = layer.weight.grad\n",
    "                        grad_bias = grad_bias.reshape(-1, 1)\n",
    "                        grads = torch.cat([grad_weights, grad_bias], dim=1)\n",
    "                    else:\n",
    "                        grads = layer.weight.grad\n",
    "                    influence_src_grads[layer].append(torch.flatten(grads))\n",
    "\n",
    "            # Calculate influences by batch to save memory\n",
    "            for layer in layer_modules:\n",
    "                query_grad_matrix = torch.stack(query_grads[layer], dim=0)\n",
    "                influence_src_grad_matrix = torch.stack(influence_src_grads[layer], dim=0)\n",
    "                tinf = torch.matmul(query_grad_matrix, torch.t(influence_src_grad_matrix))\n",
    "                tinf = tinf.detach().cpu()\n",
    "                if layer not in influences:\n",
    "                    influences[layer] = tinf\n",
    "                else:\n",
    "                    influences[layer] = torch.cat((influences[layer], tinf), dim=1)\n",
    "                influence_src_grads[layer] = []\n",
    "                \n",
    "        return influences\n",
    "            \n",
    "\n",
    "    def _compute_EKFAC_params(self, n_samples: int = 2):\n",
    "        ekfac = EKFACDistilled(self.module, 1e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "        for _, (input, _) in tqdm.tqdm(enumerate(self.cov_src_dataloader), total=len(self.cov_src_dataloader)):\n",
    "            input = input.to(self.device)\n",
    "            outputs = self.module(input)\n",
    "            output_probs = torch.softmax(outputs, dim=-1)\n",
    "            distribution = dist.Categorical(output_probs)\n",
    "            for _ in range(n_samples):\n",
    "                samples = distribution.sample()\n",
    "                loss = loss_fn(outputs, samples)\n",
    "                loss.backward(retain_graph=True)\n",
    "                ekfac.step()\n",
    "                self.module.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        G_list = {}\n",
    "        # Compute average A and S\n",
    "        for group in ekfac.param_groups:\n",
    "            G_list[group['mod']] = {}\n",
    "            with autocast():\n",
    "                A = torch.stack(group['A']).mean(dim=0)\n",
    "                S = torch.stack(group['S']).mean(dim=0)\n",
    "\n",
    "                print(f'Activation cov matrix shape {A.shape}')\n",
    "                print(f'Layer output cov matrix shape {S.shape}')\n",
    "            \n",
    "                # Compute eigenvalues and eigenvectors of A and S\n",
    "                la, Qa = torch.linalg.eigh(A)\n",
    "                ls, Qs = torch.linalg.eigh(S)\n",
    "\n",
    "                eigenval_diags = torch.outer(la, ls).flatten(start_dim=0)\n",
    "\n",
    "            G_list[group['mod']]['Qa'] = Qa\n",
    "            G_list[group['mod']]['Qs'] = Qs\n",
    "            G_list[group['mod']]['lambda'] = eigenval_diags\n",
    "            \n",
    "        return G_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN\n",
      "**********************\n",
      "Linear\n",
      "**********************\n",
      "Linear\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "precond = EKFACDistilled(net, eps=0.001)\n",
    "influence = EKFACInfluence(net, layers=['fc1', 'fc2'], influence_src_dataset=train_dataset, activation_dir='activations', model_id='test', batch_size=64, cov_batch_size=64)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for mod in net.modules():\n",
    "  mod_class = mod.__class__.__name__\n",
    "  print(mod_class)\n",
    "  print(\"**********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_list = influence._compute_EKFAC_params(n_samples=3)\n",
    "# print(G_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:12<00:00, 62.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation cov matrix shape torch.Size([785, 785])\n",
      "Layer output cov matrix shape torch.Size([256, 256])\n",
      "Activation cov matrix shape torch.Size([257, 257])\n",
      "Layer output cov matrix shape torch.Size([10, 10])\n",
      "Cacultating query gradients on trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 144.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cacultating training src gradients on trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [05:10<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "test_dataset = Subset(train_dataset, range(500))\n",
    "influences = influence.influence(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "torch.Size([500, 48000])\n",
      "torch.Size([48000])\n",
      "tensor(42815)\n",
      "tensor(14462)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "torch.Size([500, 48000])\n",
      "torch.Size([48000])\n",
      "tensor(20606)\n",
      "tensor(32383)\n"
     ]
    }
   ],
   "source": [
    "for layer in influences:\n",
    "    print(layer)\n",
    "    print(influences[layer].shape)\n",
    "    print(influences[layer][0].shape)\n",
    "    print(torch.argmax(influences[layer][0]))\n",
    "    print(torch.argmax(influences[layer][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 92.3462, 102.3696, 123.3151, 138.2845, 136.4028, 121.2249, 125.1984,\n",
      "        132.8768, 138.6667, 141.0410])\n",
      "tensor(1094.2822)\n",
      "top influence found in 39560 steps\n",
      "tensor([ 6.8856, 25.6878, 31.0755, 35.0516, 36.1526, 32.7021, 32.3929, 34.3370,\n",
      "        34.1743, 38.2346])\n",
      "tensor(309.4397)\n",
      "top influence found in 39432 steps\n",
      "tensor([17.4726, 23.1118, 59.5723, 61.5336, 61.4453, 52.9404, 54.4008, 55.4892,\n",
      "        57.8857, 58.9593])\n",
      "tensor(417.6404)\n",
      "top influence found in 33857 steps\n",
      "tensor([11.0698, 14.8960, 16.8871, 41.4456, 42.2462, 36.1389, 36.4174, 36.4267,\n",
      "        38.9356, 48.7795])\n",
      "tensor(339.2998)\n",
      "top influence found in 34559 steps\n",
      "tensor([-1.7399, -0.9295, -1.7136, -2.1980, 18.2154, 20.8756, 20.6055, 19.1498,\n",
      "        17.8205, 19.0587])\n",
      "tensor(82.7157)\n",
      "top influence found in 12321 steps\n",
      "tensor([-21.4126, -27.4272, -45.2204, -58.9710, -54.0380, -12.9528, -14.3238,\n",
      "        -16.1053, -21.8222, -22.7923])\n",
      "tensor(218.2907)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\EKFAC-Influence-Benchmarks\\notebooks\\Model_Eval.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/EKFAC-Influence-Benchmarks/notebooks/Model_Eval.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         influence[top] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/EKFAC-Influence-Benchmarks/notebooks/Model_Eval.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/EKFAC-Influence-Benchmarks/notebooks/Model_Eval.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         top \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49margmax(influence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/EKFAC-Influence-Benchmarks/notebooks/Model_Eval.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtop influence found in \u001b[39m\u001b[39m{\u001b[39;00mcount\u001b[39m}\u001b[39;00m\u001b[39m steps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/EKFAC-Influence-Benchmarks/notebooks/Model_Eval.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "store_mnist='../data'\n",
    "train_dataset = datasets.MNIST(root=store_mnist, train=True, download=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for layer in influences:\n",
    "    test_influences = influences[layer].detach().clone()\n",
    "    for i, influence in enumerate(test_influences):\n",
    "        print(influence[:10])\n",
    "        print(torch.max(influence))\n",
    "        top = torch.argmax(influence)\n",
    "        influence[top] = 0\n",
    "        count = 0\n",
    "        while top != i:\n",
    "            influence[top] = 0\n",
    "            count += 1\n",
    "            top = torch.argmax(influence)\n",
    "        print(f\"top influence found in {count} steps\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('img_proc_proj0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f16748fa5b9c7c97742513131751f6102eff5e936588c180718494a87f90c77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
