{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWuSel7j0q30",
        "outputId": "9085daa0-34f2-4b16-a63d-4e1318dac322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of x: (569, 30)\n",
            "shape of y: (569,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "x = data['data']\n",
        "y = data['target']\n",
        "print(\"shape of x: {}\\nshape of y: {}\".format(x.shape,y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "TL_ucBKL0wp8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "FGiJ0k8v00qi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32)\n",
        "    self.y = torch.tensor(y,dtype=torch.float32)\n",
        "    self.length = self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "trainset = dataset(x,y)\n",
        "#DataLoader\n",
        "trainloader = DataLoader(trainset,batch_size=60,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "wuaaUbt97Ghg"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_shape,32)\n",
        "    self.fc2 = nn.Linear(32,64)\n",
        "    self.fc3 = nn.Linear(64,1)\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "kibxNxZR89YB"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "network = Net(x.shape[1])\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "AKKZs6Py9Q3-"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "class EKFACDistilled(Optimizer):\n",
        "    def __init__(self, net, eps):\n",
        "        self.eps = eps\n",
        "        self.params = []\n",
        "        self._fwd_handles = []\n",
        "        self._bwd_handles = []\n",
        "        self.net = net\n",
        "        for mod in net.modules():\n",
        "            mod_class = mod.__class__.__name__\n",
        "            if mod_class in ['Linear']:\n",
        "                handle = mod.register_forward_pre_hook(self._save_input)\n",
        "                self._fwd_handles.append(handle)\n",
        "                handle = mod.register_full_backward_hook(self._save_grad_output)\n",
        "                self._bwd_handles.append(handle)\n",
        "                params = [mod.weight]\n",
        "                if mod.bias is not None:\n",
        "                    params.append(mod.bias)\n",
        "                d = {'params': params, 'mod': mod, 'layer_type': mod_class, 'A': [], 'S': []}\n",
        "                self.params.append(d)\n",
        "        super(EKFACDistilled, self).__init__(self.params, {})\n",
        "\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            if len(group['params']) == 2:\n",
        "                weight, bias = group['params']\n",
        "            else:\n",
        "                weight = group['params'][0]\n",
        "                bias = None\n",
        "            state = self.state[weight]\n",
        "\n",
        "            self._compute_kfe(group, state)\n",
        "\n",
        "            self._precond(weight, bias, group, state)\n",
        "\n",
        "    def calc_cov(self, calc_act: bool = True):\n",
        "        for group in self.param_groups:\n",
        "            if len(group['params']) == 2:\n",
        "                weight, bias = group['params']\n",
        "            else:\n",
        "                weight = group['params'][0]\n",
        "                bias = None\n",
        "\n",
        "            state = self.state[weight]\n",
        "\n",
        "            mod = group['mod']\n",
        "            x = self.state[group['mod']]['x']\n",
        "            gy = self.state[group['mod']]['gy']\n",
        "\n",
        "            # Computation of activation cov matrix for batch\n",
        "            x = x.data.t()\n",
        "\n",
        "            # Append column of ones to x if bias is not None\n",
        "            if mod.bias is not None:\n",
        "                ones = torch.ones_like(x[:1])\n",
        "                x = torch.cat([x, ones], dim=0)\n",
        "            \n",
        "            if calc_act:\n",
        "                # Calculate covariance matrix for activations (A_{l-1})\n",
        "                A = torch.mm(x, x.t()) / float(x.shape[1])\n",
        "                group['A'].append(A)\n",
        "\n",
        "            # Computation of psuedograd of layer output cov matrix for batch\n",
        "            gy = gy.data.t()\n",
        "\n",
        "            # Calculate covariance matrix for layer outputs (S_{l})\n",
        "            S = torch.mm(gy, gy.t()) / float(gy.shape[1])\n",
        "\n",
        "            group['S'].append(S)\n",
        "\n",
        "    def _compute_kfe(self, group, state):\n",
        "        mod = group['mod']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        \n",
        "        # Computation of xxt\n",
        "        x = x.data.t() # transpose of activations\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if mod.bias is not None:\n",
        "            ones = torch.ones_like(x[:1])\n",
        "            x = torch.cat([x, ones], dim=0)\n",
        "\n",
        "        # Calculate covariance matrix for activations (A_{l-1})\n",
        "        xxt = torch.mm(x, x.t()) / float(x.shape[1])\n",
        "\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaA, QA)\n",
        "        la, state['Qa'] = torch.linalg.eigh(xxt, UPLO='U')\n",
        "\n",
        "        # Computation of ggt\n",
        "        gy = gy.data.t()\n",
        "\n",
        "        # Calculate covariance matrix for layer outputs (S_{l})\n",
        "        ggt = torch.mm(gy, gy.t()) / float(gy.shape[1])\n",
        "\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaS, QS)\n",
        "        ls, state['Qs'] = torch.linalg.eigh(ggt, UPLO='U')\n",
        "\n",
        "        # Outer product of the eigenvalue vectors. Of shape (len(s) x len(a))\n",
        "        state['m2'] = ls.unsqueeze(1) * la.unsqueeze(0)\n",
        "\n",
        "    def _precond(self, weight, bias, group, state):\n",
        "        \"\"\"Applies preconditioning.\"\"\"\n",
        "        Qa = state['Qa']\n",
        "        Qs = state['Qs']\n",
        "        m2 = state['m2']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        g = weight.grad.data\n",
        "        s = g.shape\n",
        "        s_x = x.size()\n",
        "        s_gy = gy.size()\n",
        "        bs = x.size(0)\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if bias is not None:\n",
        "            ones = torch.ones_like(x[:,:1])\n",
        "            x = torch.cat([x, ones], dim=1)\n",
        "        \n",
        "        # KFE of activations ??\n",
        "        x_kfe = torch.mm(x, Qa)\n",
        "\n",
        "        # KFE of layer outputs ??\n",
        "        gy_kfe = torch.mm(gy, Qs)\n",
        "\n",
        "        m2 = torch.mm(gy_kfe.t()**2, x_kfe**2) / bs\n",
        "\n",
        "        g_kfe = torch.mm(gy_kfe.t(), x_kfe) / bs\n",
        "\n",
        "        g_nat_kfe = g_kfe / (m2 + self.eps)\n",
        "\n",
        "        g_nat = torch.mm(g_nat_kfe, Qs.t())\n",
        "\n",
        "        if bias is not None:\n",
        "            gb = g_nat[:, -1].contiguous().view(*bias.shape)\n",
        "            bias.grad.data = gb\n",
        "            g_nat = g_nat[:, :-1]\n",
        "        \n",
        "        g_nat = g_nat.contiguous().view(*s)\n",
        "        weight.grad.data = g_nat\n",
        "\n",
        "    def _save_input(self, mod, i):\n",
        "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['x'] = i[0]\n",
        "\n",
        "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
        "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import captum._utils.common as common\n",
        "from captum.influence._core.influence import DataInfluence\n",
        "from torch.nn import Module\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from torch import Tensor\n",
        "\n",
        "class EKFACInfluence(DataInfluence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        module: Module,\n",
        "        layers: Union[str, List[str]],\n",
        "        influence_src_dataset: Dataset,\n",
        "        activation_dir: str,\n",
        "        model_id: str = \"\",\n",
        "        batch_size: int = 1,\n",
        "        query_batch_size: int = 1,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            module (Module): An instance of pytorch model. This model should define all of its\n",
        "                layers as attributes of the model. The output of the model must be logits for the\n",
        "                classification task.\n",
        "            layers (Union[str, List[str]]): A list of layer names for which the influence will\n",
        "                be computed.\n",
        "            influence_src_dataset (torch.utils.data.Dataset): Pytorch dataset that is used to create\n",
        "                a pytorch dataloader to iterate over the dataset. This is the dataset for which we will\n",
        "                be seeking for influential instances. In most cases this is the training dataset.\n",
        "            activation_dir (str): Path to the directory where the activation computations will be stored.\n",
        "            model_id (str): The name/version of the model for which layer activations are being computed.\n",
        "                Activations will be stored and loaded under the subdirectory with this name if provided.\n",
        "            batch_size (int): Batch size for the dataloader used to iterate over the influence_src_dataset.\n",
        "            **kwargs: Any additional arguments that are necessary for specific implementations of the\n",
        "                'DataInfluence' abstract class.\n",
        "        \"\"\"\n",
        "        self.module = module\n",
        "        self.layers = [layers] if isinstance(layers, str) else layers\n",
        "        self.influence_src_dataset = influence_src_dataset\n",
        "        self.activation_dir = activation_dir\n",
        "        self.model_id = model_id\n",
        "        self.batch_size = batch_size\n",
        "        self.query_batch_size = query_batch_size\n",
        "\n",
        "        self.influence_src_dataloader = DataLoader(\n",
        "            self.influence_src_dataset, batch_size=batch_size, shuffle=False\n",
        "        )\n",
        "    \n",
        "    def influence(\n",
        "            self,\n",
        "            inputs: Dataset,\n",
        "            topk: int = 1,\n",
        "            additional_forward_args: Optional[Any] = None,\n",
        "            load_src_from_disk: bool = True,\n",
        "            **kwargs: Any,\n",
        "        ) -> Dict:\n",
        "\n",
        "        inputs_batch_size = (\n",
        "            inputs[0].shape[0] if isinstance(inputs, tuple) else inputs.shape[0]\n",
        "        )\n",
        "\n",
        "        influences: Dict[str, Any] = {}\n",
        "        query_grads: Dict[str, List[Tensor]] = {}\n",
        "        influence_src_grads: Dict[str, List[Tensor]] = {}\n",
        "\n",
        "        query_dataloader = DataLoader(\n",
        "            inputs, batch_size=self.query_batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        layer_modules = [\n",
        "            common._get_module_from_name(self.module, layer) for layer in self.layers\n",
        "        ]\n",
        "\n",
        "        G_list = self._compute_EKFAC_GNH()\n",
        "\n",
        "        for i, (queries, targets) in enumerate(query_dataloader):\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "            self.module.zero_grad()\n",
        "            queries, targets = inputs\n",
        "            outputs = self.module(queries)\n",
        "            loss = criterion(outputs, targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            for layer in layer_modules:\n",
        "                if layer.bias is not None:\n",
        "                    grad_bias = layer.bias.grad\n",
        "                    grad_weights = layer.weight.grad\n",
        "                    grads = torch.cat([grad_weights.view(-1), grad_bias.view(-1)], dim=1)\n",
        "                else:\n",
        "                    grads = layer.weight.grad.view(-1)\n",
        "                for grad in grads:\n",
        "                    query_grads[layer].append(grad)\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(self.influence_src_dataloader):\n",
        "            self.module.zero_grad()\n",
        "            outputs = self.module(inputs)\n",
        "            loss = criterion(outputs, targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            for layer in layer_modules:\n",
        "                if layer.bias is not None:\n",
        "                    grad_bias = layer.bias.grad\n",
        "                    grad_weights = layer.weight.grad\n",
        "                    grads = torch.cat([grad_weights.view(-1), grad_bias.view(-1)], dim=1)\n",
        "                else:\n",
        "                    grads = layer.weight.grad.view(-1)\n",
        "                for grad in grads:\n",
        "                    influence_src_grads[layer].append(grad)\n",
        "        \n",
        "        for layer in layer_modules:\n",
        "            query_grads[layer] = torch.stack(query_grads[layer])\n",
        "            influence_src_grads[layer] = torch.stack(influence_src_grads[layer])\n",
        "            influences[layer] = torch.matmul(influence_src_grads[layer], torch.matmul(G_list[layer], query_grads[layer]).t())\n",
        "\n",
        "        return influences\n",
        "            \n",
        "\n",
        "    def _compute_EKFAC_GNH(self, n_samples: int = 2):\n",
        "        ekfac = EKFACDistilled(self.module, 1e-5)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "        for i, (input, _) in enumerate(self.influence_src_dataloader):\n",
        "            outputs = self.module(input)\n",
        "            output_probs = torch.softmax(outputs, dim=-1)\n",
        "            distribution = dist.Categorical(output_probs)\n",
        "            for j in range(n_samples):\n",
        "                samples = distribution.sample()\n",
        "                loss = loss_fn(outputs, samples)\n",
        "                loss.backward()\n",
        "                ekfac.step()\n",
        "                self.module.zero_grad()\n",
        "        \n",
        "        G_list = []\n",
        "        # Compute average A and S\n",
        "        for group in ekfac.param_groups:\n",
        "            A = torch.stack(group['A']).mean(dim=0)\n",
        "            S = torch.stack(group['S']).mean(dim=0)\n",
        "        \n",
        "            # Compute eigenvalues and eigenvectors of A and S\n",
        "            la, Qa = torch.linalg.eigh(A, UPLO='U')\n",
        "            ls, Qs = torch.linalg.eigh(S, UPLO='U')\n",
        "\n",
        "            # Compute Kronecker product of eigenvalues and eigenvectors\n",
        "            eigenvec_kron = torch.kron(Qa, Qs)\n",
        "\n",
        "            eigenval_kron = torch.kron(torch.diag(la),torch.diag(ls))\n",
        "\n",
        "            # Compute GNH\n",
        "            G_list.append(torch.matmul(eigenvec_kron, torch.matmul(eigenval_kron, eigenvec_kron.t())))\n",
        "            \n",
        "        return G_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcuk4RoJ9dIM",
        "outputId": "6096bb71-01cb-4934-e929-35ed660117e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n"
          ]
        }
      ],
      "source": [
        "precond = EKFACDistilled(network, eps=0.001)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "for mod in network.modules():\n",
        "  mod_class = mod.__class__.__name__\n",
        "  print(mod_class)\n",
        "  print(\"**********************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YmB6iG_9dyA",
        "outputId": "7e70ef48-d35e-4646-8792-77ffcb4dbdcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[ 1.0971, -2.0733,  1.2699,  ...,  2.2961,  2.7506,  1.9370],\n",
            "        [ 1.8298, -0.3536,  1.6860,  ...,  1.0871, -0.2439,  0.2812],\n",
            "        [ 1.5799,  0.4562,  1.5665,  ...,  1.9550,  1.1523,  0.2014],\n",
            "        ...,\n",
            "        [ 0.1655,  0.5353,  0.1475,  ...,  1.0475,  1.2898,  1.4106],\n",
            "        [-0.3060,  0.0047, -0.3855,  ..., -1.5759, -0.7470, -1.1668],\n",
            "        [-1.5647, -1.7452, -1.5499,  ..., -1.0722,  0.5165,  0.3499]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-1.1239, -1.0262, -1.1294,  ..., -1.3524,  1.0617, -0.2076],\n",
            "        [-1.5704,  0.3934, -1.5368,  ..., -1.3221,  0.1153, -0.3783],\n",
            "        [ 0.0349,  0.6656,  0.1833,  ...,  0.9729, -0.0918,  1.6211],\n",
            "        ...,\n",
            "        [ 0.2109, -0.6096,  0.2748,  ...,  1.3414,  1.1070,  1.2499],\n",
            "        [ 0.4694,  0.8425,  0.5656,  ...,  1.3520,  0.6038,  2.2861],\n",
            "        [ 1.0857,  0.1676,  0.9157,  ...,  0.0593,  3.2052, -1.2655]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-0.7717, -1.9709, -0.7673,  ..., -0.3811,  0.1864,  0.0712],\n",
            "        [ 1.2874, -0.5049,  1.2123,  ...,  0.8039, -0.0109,  0.0340],\n",
            "        [ 2.8750,  0.2118,  3.0576,  ...,  1.6779,  0.5197, -0.2137],\n",
            "        ...,\n",
            "        [ 0.6625,  0.1909,  0.7139,  ...,  1.3535,  0.2479,  0.6231],\n",
            "        [-0.3173,  0.6819, -0.4102,  ..., -1.6041, -0.9800, -1.4140],\n",
            "        [-0.3741, -1.4497, -0.4399,  ..., -1.1375, -1.8083, -0.5960]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[ 3.7185,  0.6005,  3.7125,  ...,  2.3478, -0.0724, -0.1732],\n",
            "        [ 1.9775,  1.6942,  2.0896,  ...,  2.6752,  1.9369,  2.4635],\n",
            "        [ 0.4467,  0.2374,  0.3802,  ...,  0.6014,  0.8675,  0.1310],\n",
            "        ...,\n",
            "        [ 1.8043,  0.5051,  1.6695,  ...,  0.4552, -1.0722, -0.7024],\n",
            "        [ 0.0263,  1.9921,  0.0239,  ..., -0.4936, -1.6352, -0.3317],\n",
            "        [ 0.9465,  4.6519,  0.8827,  ...,  1.4008, -0.0773,  0.0562]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-1.3840e-01, -8.5861e-01, -1.8902e-01,  ..., -3.4087e-01,\n",
            "         -5.9980e-01, -1.0444e+00],\n",
            "        [-4.8490e-01, -9.8892e-01, -5.5026e-01,  ..., -1.1281e+00,\n",
            "          3.9520e-04, -8.9307e-01],\n",
            "        [-8.0299e-01, -2.5590e-01, -7.4303e-01,  ...,  1.0954e-01,\n",
            "          6.5883e-01,  2.5355e+00],\n",
            "        ...,\n",
            "        [-6.7234e-01, -2.6753e-01, -6.9896e-01,  ..., -6.5484e-01,\n",
            "         -1.4928e+00, -8.1992e-01],\n",
            "        [ 3.7691e-02, -2.6055e-01, -3.0853e-02,  ..., -5.9850e-01,\n",
            "         -4.2832e-01, -3.9821e-01],\n",
            "        [-1.0274e+00,  8.8437e-01, -1.0347e+00,  ..., -1.2692e+00,\n",
            "         -1.0900e+00, -8.9640e-01]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[ 1.5344, -0.0907,  1.5459,  ...,  1.2698,  0.1088,  0.8503],\n",
            "        [-0.4735,  0.1397, -0.4753,  ..., -0.5840, -0.3490, -0.3494],\n",
            "        [ 1.6935,  1.0636,  1.7601,  ...,  1.1830,  0.6362,  0.5954],\n",
            "        ...,\n",
            "        [-0.0731, -0.7167, -0.1421,  ..., -0.8991, -0.8716, -0.7102],\n",
            "        [-1.4909, -0.8842, -1.4511,  ..., -1.0200, -0.7551, -0.5340],\n",
            "        [-1.3324, -0.2256, -1.3242,  ..., -0.9758, -0.7228, -0.1433]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-0.4508, -0.2838, -0.5169,  ..., -1.4961, -1.0803, -1.5924],\n",
            "        [-0.2350,  0.5307, -0.2772,  ..., -0.8902, -0.4267, -0.9623],\n",
            "        [-0.3883, -0.1046, -0.4160,  ..., -0.4794, -0.2536, -0.6409],\n",
            "        ...,\n",
            "        [ 0.3899,  0.4166,  0.4502,  ...,  1.5043,  0.1654,  1.1557],\n",
            "        [-0.4054, -1.6568, -0.4568,  ..., -0.4928, -0.2034,  0.5926],\n",
            "        [-0.8428,  0.4934, -0.8658,  ..., -1.0894,  0.1217, -0.7047]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-0.7263, -0.0581, -0.7319,  ..., -0.7304,  0.2172, -0.0613],\n",
            "        [ 0.1598, -1.2356,  0.2575,  ..., -0.0580, -0.1193,  0.4508],\n",
            "        [-0.7149, -0.7609, -0.6800,  ..., -0.0625, -0.1840, -0.5362],\n",
            "        ...,\n",
            "        [-0.0646, -0.6212, -0.1235,  ..., -0.7977, -0.3588, -0.3860],\n",
            "        [-0.7490, -1.0936, -0.7406,  ..., -0.6136,  0.0651,  0.4352],\n",
            "        [ 0.6029,  0.0513,  0.7345,  ...,  0.9577,  0.6750,  0.4109]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([60, 30])\n",
            "tensor([[-0.5587, -0.2931, -0.5634,  ..., -0.8787, -0.8004, -0.3688],\n",
            "        [-0.0646, -0.0116, -0.1334,  ..., -0.5041, -0.8813, -0.4387],\n",
            "        [-0.1867, -1.2170, -0.1915,  ...,  0.2877,  0.5278,  0.5162],\n",
            "        ...,\n",
            "        [-0.6922,  1.1985, -0.6425,  ...,  0.2466, -0.1581,  0.8730],\n",
            "        [-1.8172,  1.4429, -1.8119,  ..., -1.7451,  0.2544,  0.8553],\n",
            "        [-1.8280,  1.4312, -1.7971,  ..., -0.9837, -0.1792,  1.2554]])\n",
            "Target Shape: torch.Size([60])\n",
            "Output Shape: torch.Size([60, 1])\n",
            "Input Shape: torch.Size([29, 30])\n",
            "tensor([[-7.3483e-01, -1.1285e+00, -7.1337e-01, -7.1668e-01,  2.4764e-01,\n",
            "          1.4515e-01, -2.6904e-01, -5.9272e-01,  2.3298e-02,  7.1198e-01,\n",
            "         -4.5755e-01,  9.9997e-01, -6.1288e-01, -4.2853e-01,  1.7031e+00,\n",
            "          8.7422e-01,  7.8371e-01,  5.0996e-01, -2.5939e-01,  6.4949e-01,\n",
            "         -8.3023e-01, -9.7661e-01, -8.4834e-01, -7.4322e-01,  9.3432e-02,\n",
            "         -2.7014e-01, -4.4372e-01, -6.9169e-01, -9.2498e-01, -1.4440e-01],\n",
            "        [ 9.7334e-02,  1.3265e+00,  1.5821e-01,  4.2971e-03, -5.6863e-01,\n",
            "          3.5362e-01,  1.5192e-01, -2.5843e-01,  2.2045e-01,  8.6813e-02,\n",
            "         -5.4489e-01, -2.5011e-01, -1.2428e-01, -3.7903e-01,  3.2342e-02,\n",
            "          1.1766e+00,  2.1208e-01, -2.8571e-02,  1.6673e-02,  8.7607e-01,\n",
            "         -1.0186e-02,  9.8566e-01,  1.8583e-01, -1.2601e-01,  7.1514e-02,\n",
            "          1.0556e+00,  6.3237e-01,  8.9742e-02,  4.6308e-01,  1.0171e+00],\n",
            "        [ 1.7402e-01,  1.4266e+00,  1.1249e-01,  3.8995e-02, -9.6858e-01,\n",
            "         -6.1026e-01, -5.9949e-01, -4.8104e-01,  1.0362e-01, -8.5022e-01,\n",
            "         -3.6840e-01,  3.0508e-01, -3.4111e-01, -2.8442e-01, -7.5536e-01,\n",
            "         -7.6894e-01, -4.1193e-01,  1.4499e-01, -2.2306e-01, -4.4214e-01,\n",
            "          4.9868e-02,  1.0768e+00,  4.1339e-03, -9.5249e-02, -1.1559e+00,\n",
            "         -7.4215e-01, -5.3295e-01, -7.7750e-02, -2.8919e-01, -7.9720e-01],\n",
            "        [-2.6052e-01,  2.0409e+00, -2.9200e-01, -3.3131e-01, -6.8677e-01,\n",
            "         -6.7412e-01, -7.3986e-01, -4.1707e-01, -6.7038e-01, -7.0705e-01,\n",
            "         -6.1383e-01,  6.8972e-01, -6.5693e-01, -4.9475e-01, -6.8935e-01,\n",
            "         -6.5716e-01, -5.6081e-01, -4.3458e-01, -3.9984e-01, -9.2744e-01,\n",
            "         -3.9329e-01,  1.8715e+00, -4.4027e-01, -4.4121e-01, -1.1033e+00,\n",
            "         -7.3897e-01, -7.9633e-01, -5.3333e-01, -6.9201e-01, -1.0815e+00],\n",
            "        [-7.3075e-02,  3.2820e-01, -9.0579e-02, -1.9934e-01, -4.1296e-02,\n",
            "         -4.8155e-02, -6.5185e-01, -6.5076e-01, -6.9959e-01,  5.7872e-01,\n",
            "         -4.8065e-01, -3.0817e-01, -3.9110e-01, -3.7881e-01, -2.4767e-01,\n",
            "         -2.1004e-01, -1.9044e-01, -4.4367e-01, -6.8316e-01, -7.4101e-02,\n",
            "         -2.5247e-01, -1.5099e-01, -2.4100e-01, -3.3749e-01, -2.6164e-01,\n",
            "         -3.2166e-01, -6.4521e-01, -7.0280e-01, -1.0544e+00,  5.3985e-02],\n",
            "        [-1.4408e-01,  9.1695e-01, -1.9685e-01, -2.3233e-01, -2.7757e-01,\n",
            "         -6.9876e-01, -7.4149e-01, -6.3167e-01, -5.3895e-01, -6.7869e-01,\n",
            "         -2.1356e-01,  2.1617e-01, -3.9605e-01, -2.0015e-01, -3.9101e-01,\n",
            "         -2.5084e-01, -3.8740e-01, -4.4318e-01,  3.9678e-02, -4.5840e-01,\n",
            "         -1.9035e-01,  5.5575e-01, -2.8836e-01, -2.6506e-01, -4.7205e-01,\n",
            "         -6.5246e-01, -8.0257e-01, -6.5271e-01, -4.1861e-01, -7.9886e-01],\n",
            "        [-1.0813e+00, -6.8408e-01, -1.0981e+00, -9.3852e-01, -1.4377e-01,\n",
            "         -1.0310e+00, -9.8782e-01, -1.1201e+00,  2.6791e-01, -1.1165e-01,\n",
            "         -7.0297e-01, -4.5332e-01, -7.4753e-01, -6.0212e-01,  1.5008e-02,\n",
            "         -1.0189e+00, -7.2195e-01, -1.0221e+00, -5.9841e-01, -4.4970e-01,\n",
            "         -1.0394e+00, -6.3627e-01, -1.0765e+00, -8.7137e-01, -1.6958e-01,\n",
            "         -1.0550e+00, -1.0955e+00, -1.3825e+00, -3.5552e-01, -5.5171e-01],\n",
            "        [-1.0984e+00, -6.3055e-01, -1.0758e+00, -9.5018e-01, -5.4017e-01,\n",
            "         -4.4879e-01, -5.6773e-01, -6.3296e-01, -5.2069e-01,  6.1558e-01,\n",
            "         -1.0495e+00, -3.5172e-01, -9.2935e-01, -7.2630e-01,  1.0764e+00,\n",
            "          2.9967e-01, -1.9111e-01, -1.3401e-01,  2.6973e-01,  7.9247e-01,\n",
            "         -1.1264e+00, -5.9230e-01, -1.0777e+00, -9.1971e-01,  6.0193e-01,\n",
            "         -1.8871e-01, -4.5043e-01, -4.7623e-01, -3.3934e-01,  6.0094e-01],\n",
            "        [-1.2622e+00,  1.1717e-02, -1.2736e+00, -1.0500e+00, -8.1486e-01,\n",
            "         -1.0242e+00, -8.2146e-01, -1.0138e+00, -8.4563e-01, -6.3453e-02,\n",
            "         -3.9511e-01,  2.6516e-01, -4.0199e-01, -4.8617e-01,  1.3301e-01,\n",
            "         -7.9632e-01, -2.8262e-01, -3.5380e-01,  1.8013e-01,  1.3583e-01,\n",
            "         -1.1057e+00, -1.4204e-02, -1.1367e+00, -9.0776e-01, -5.4657e-01,\n",
            "         -1.0102e+00, -8.5726e-01, -1.1594e+00, -5.6421e-01, -2.6299e-01],\n",
            "        [-9.3932e-01,  1.1450e+00, -9.5063e-01, -8.3414e-01, -1.0276e+00,\n",
            "         -7.2624e-01, -9.2052e-01, -1.0513e+00,  6.0015e-01,  6.8384e-02,\n",
            "          4.1299e-01,  1.2721e+00,  3.4550e-01, -1.6143e-01,  4.0736e-01,\n",
            "         -3.7883e-01, -6.3409e-01, -9.5365e-01,  4.9857e-01, -3.0937e-01,\n",
            "         -6.7078e-01,  9.4006e-01, -6.9583e-01, -6.5919e-01, -5.2465e-01,\n",
            "         -5.7866e-01, -1.0087e+00, -1.2481e+00,  2.5600e-01, -4.2592e-01],\n",
            "        [-9.2796e-01,  5.0971e-01, -9.6628e-01, -8.3727e-01, -1.5692e+00,\n",
            "         -1.1763e+00, -1.1149e+00, -1.2618e+00, -5.4990e-01, -4.7031e-01,\n",
            "         -3.2076e-01,  1.5811e-01, -3.7180e-01, -4.3271e-01,  8.4604e-01,\n",
            "         -8.0694e-01, -1.0575e+00, -1.9134e+00,  1.1500e+00, -5.9268e-01,\n",
            "         -9.5448e-01, -1.4774e-01, -9.8833e-01, -8.2320e-01, -1.4145e+00,\n",
            "         -1.1500e+00, -1.3058e+00, -1.7451e+00, -7.1628e-01, -9.9892e-01],\n",
            "        [-8.5127e-01,  7.3311e-01, -8.4354e-01, -7.8636e-01, -4.9836e-02,\n",
            "         -4.2453e-01, -5.0922e-01, -6.7965e-01,  7.9730e-01,  3.8593e-01,\n",
            "         -4.5177e-01,  4.5385e-01, -4.3170e-01, -4.9475e-01, -1.1820e+00,\n",
            "          2.8123e-01,  8.4759e-02, -2.5242e-01,  1.0386e+00,  3.5105e-01,\n",
            "         -8.7993e-01,  4.2059e-01, -8.7753e-01, -7.8048e-01, -1.0375e+00,\n",
            "         -4.8388e-01, -5.5550e-01, -7.6858e-01,  4.3396e-01, -2.0093e-01],\n",
            "        [-3.8549e-01,  2.3597e+00, -4.3740e-01, -4.1805e-01, -9.6787e-01,\n",
            "         -1.1750e+00, -8.6415e-01, -8.7517e-01, -9.9532e-01, -9.1118e-01,\n",
            "         -5.9289e-01,  2.7242e-01, -6.8763e-01, -4.7473e-01,  5.9803e-01,\n",
            "         -7.3484e-01, -6.1718e-01, -4.0409e-01, -1.9037e-01, -7.8257e-01,\n",
            "         -4.9683e-01,  1.6810e+00, -5.7073e-01, -5.0256e-01, -3.9315e-01,\n",
            "         -9.4063e-01, -8.9070e-01, -7.5564e-01, -7.9879e-01, -1.0588e+00],\n",
            "        [-1.3616e+00,  6.1675e-01, -1.3576e+00, -1.1117e+00, -2.8184e-01,\n",
            "         -9.1519e-01, -6.1318e-01, -9.3114e-01, -4.3672e-01,  4.1995e-01,\n",
            "         -3.7490e-01,  1.2014e+00, -3.6883e-01, -4.9453e-01,  1.2997e+00,\n",
            "         -3.9895e-01,  2.6745e-01,  1.6608e-01,  2.0641e+00,  3.1323e-01,\n",
            "         -1.3303e+00, -1.0214e-01, -1.3225e+00, -1.0280e+00, -9.6740e-01,\n",
            "         -1.0896e+00, -9.2236e-01, -1.3547e+00, -7.5349e-01, -5.5503e-01],\n",
            "        [-3.5425e-01,  2.2410e+00, -3.9003e-01, -3.9985e-01, -1.0768e+00,\n",
            "         -8.7368e-01, -3.3709e-01, -6.5747e-01, -8.9674e-01, -8.1053e-01,\n",
            "         -6.9864e-01,  2.5972e-01, -6.7525e-01, -5.1720e-01,  4.5703e-01,\n",
            "         -2.2066e-01,  2.3496e-01, -6.7741e-01, -4.3495e-01, -3.7595e-01,\n",
            "         -4.9269e-01,  1.6387e+00, -5.4869e-01, -5.0080e-01, -4.2383e-01,\n",
            "         -5.8693e-01, -1.3571e-01, -7.5640e-01, -8.5541e-01, -6.3871e-01],\n",
            "        [-1.0898e+00,  1.9362e+00, -1.0833e+00, -9.4848e-01, -4.3128e-01,\n",
            "         -5.2611e-01, -3.6170e-01, -5.5558e-01, -7.9816e-01, -2.1655e-01,\n",
            "         -6.6869e-01,  1.8545e+00, -7.0743e-01, -5.6934e-01,  1.6697e+00,\n",
            "          1.0518e-01,  5.3536e-01,  8.7818e-01, -2.5575e-01,  4.3238e-01,\n",
            "         -1.1243e+00,  1.5035e+00, -1.1227e+00, -9.1936e-01,  2.6439e-01,\n",
            "         -5.2968e-01, -3.4633e-01, -3.5533e-01, -1.0916e+00, -6.1834e-02],\n",
            "        [-1.1268e+00,  6.9894e-02, -1.1220e+00, -9.7607e-01,  2.8037e-01,\n",
            "         -5.5530e-01, -1.0518e+00, -9.7396e-01, -7.5277e-02,  7.2637e-02,\n",
            "         -5.8134e-01,  1.5842e+00, -6.0298e-01, -5.1786e-01,  1.9564e+00,\n",
            "         -1.8210e-01, -9.1910e-01, -7.6468e-01,  6.2691e-01, -5.7377e-01,\n",
            "         -1.1636e+00, -4.5551e-01, -1.1730e+00, -9.3747e-01, -2.5726e-01,\n",
            "         -8.5411e-01, -1.2576e+00, -1.4052e+00, -1.0334e+00, -9.1579e-01],\n",
            "        [-1.3361e+00,  1.9990e+00, -1.3473e+00, -1.0910e+00, -1.0768e+00,\n",
            "         -1.0353e+00, -1.1149e+00, -1.2618e+00, -2.5417e-01, -3.1295e-01,\n",
            "          4.7760e-01,  3.1028e+00,  3.7223e-01, -2.4701e-01,  1.5164e+00,\n",
            "         -7.9576e-01, -1.0575e+00, -1.9134e+00,  1.1500e+00, -1.7812e-01,\n",
            "         -1.1968e+00,  1.3944e+00, -1.2141e+00, -9.6682e-01, -1.0989e+00,\n",
            "         -1.1621e+00, -1.3058e+00, -1.7451e+00, -6.8878e-01, -7.9000e-01],\n",
            "        [ 1.3142e-01,  7.8896e-01,  1.8210e-01,  6.2880e-03, -8.2767e-01,\n",
            "          5.4313e-01,  1.7703e-01, -2.9816e-01, -1.3056e+00, -1.8820e-01,\n",
            "         -6.4884e-01, -1.9750e-01, -3.1784e-01, -4.5757e-01, -9.3303e-01,\n",
            "          1.1688e+00,  1.1236e+00,  6.9164e-01, -5.0396e-01,  2.3115e-01,\n",
            "         -1.6343e-01,  2.5937e-01, -4.0545e-02, -2.5856e-01, -1.3049e+00,\n",
            "          3.9972e-01,  4.5102e-01, -6.2524e-02, -1.0398e+00, -2.1644e-01],\n",
            "        [-7.4335e-01,  1.0798e+00, -7.1873e-01, -7.1498e-01, -2.6689e-01,\n",
            "         -4.2470e-02,  2.8124e-01, -2.0298e-01, -1.5466e+00,  4.1144e-01,\n",
            "         -6.0047e-01,  3.0611e+00, -4.6041e-01, -5.1412e-01,  3.8636e-01,\n",
            "          2.4266e-01,  8.4505e-01,  1.4175e-01, -6.8558e-01,  3.5673e-01,\n",
            "         -7.8468e-01,  1.8699e+00, -7.4409e-01, -7.1439e-01, -1.1260e-01,\n",
            "         -1.6317e-02,  4.3567e-01, -2.7524e-01, -1.2760e+00,  1.8698e-01],\n",
            "        [-2.1952e-02,  1.8292e+00, -2.4262e-02, -1.5497e-01,  2.0849e-01,\n",
            "          1.5652e-01, -5.5467e-01, -1.5165e-01, -1.0026e+00, -1.5418e-01,\n",
            "         -1.4679e-01,  4.9921e-01,  1.0861e-02, -2.3095e-01,  7.1677e-02,\n",
            "          7.2760e-02, -3.7082e-01,  7.2408e-01,  3.1202e-02,  5.7082e-01,\n",
            "         -2.0070e-01,  1.2202e+00, -2.1032e-01, -3.0567e-01, -3.6246e-01,\n",
            "         -1.7726e-01, -6.6968e-01, -1.4932e-01, -1.0528e+00, -4.0776e-02],\n",
            "        [-8.3139e-01,  2.3458e+00, -8.7731e-01, -7.6475e-01, -1.5564e+00,\n",
            "         -1.3031e+00, -1.1149e+00, -1.2618e+00, -2.7441e+00, -1.1026e+00,\n",
            "         -3.2870e-01,  4.8609e+00, -4.0843e-01, -3.8563e-01,  1.8435e-01,\n",
            "         -9.2777e-01, -1.0575e+00, -1.9134e+00, -7.8979e-02, -7.6479e-01,\n",
            "         -9.0064e-01,  2.0555e+00, -9.5527e-01, -7.7521e-01, -1.7402e+00,\n",
            "         -1.2680e+00, -1.3058e+00, -1.7451e+00, -2.1593e+00, -1.3796e+00],\n",
            "        [ 3.1034e-01,  2.6366e+00,  4.7084e-01,  1.7637e-01,  6.0062e-01,\n",
            "          1.9778e+00,  2.0866e+00,  1.1703e+00,  1.1551e+00,  1.2365e+00,\n",
            "         -5.2323e-01, -2.1506e-02, -2.4952e-01, -3.8915e-01, -8.0536e-01,\n",
            "          1.2833e+00,  1.3825e+00,  6.9488e-01,  1.0022e-01,  8.8779e-01,\n",
            "          2.5902e-01,  2.7867e+00,  6.3857e-01,  6.0502e-02,  4.0905e-01,\n",
            "          3.4188e+00,  4.3073e+00,  1.8423e+00,  1.9223e+00,  3.1562e+00],\n",
            "        [ 1.9292e+00,  1.3498e+00,  2.1020e+00,  1.9684e+00,  9.6356e-01,\n",
            "          2.2601e+00,  2.8701e+00,  2.5402e+00,  1.2318e+00,  8.4948e-01,\n",
            "          2.0104e+00, -3.4627e-01,  2.9167e+00,  1.7263e+00, -2.1400e-01,\n",
            "          9.8488e-01,  1.5437e+00,  2.3429e+00,  3.3540e-03,  9.1465e-01,\n",
            "          1.6610e+00,  6.0786e-01,  2.1398e+00,  1.6497e+00,  3.6521e-01,\n",
            "          1.0454e+00,  1.8601e+00,  2.1255e+00,  4.5693e-02,  8.1928e-01],\n",
            "        [ 2.1110e+00,  7.2147e-01,  2.0608e+00,  2.3439e+00,  1.0418e+00,\n",
            "          2.1906e-01,  1.9473e+00,  2.3210e+00, -3.1259e-01, -9.3103e-01,\n",
            "          2.7821e+00,  7.1025e-02,  2.3796e+00,  2.6042e+00,  1.0864e+00,\n",
            "          1.9181e-01,  6.6600e-01,  2.0672e+00, -1.1384e+00,  1.6798e-01,\n",
            "          1.9012e+00,  1.1770e-01,  1.7526e+00,  2.0153e+00,  3.7837e-01,\n",
            "         -2.7332e-01,  6.6451e-01,  1.6292e+00, -1.3602e+00, -7.0909e-01],\n",
            "        [ 1.7049e+00,  2.0851e+00,  1.6159e+00,  1.7238e+00,  1.0246e-01,\n",
            "         -1.7833e-02,  6.9304e-01,  1.2637e+00, -2.1766e-01, -1.0586e+00,\n",
            "          1.3005e+00,  2.2609e+00,  1.1569e+00,  1.2916e+00, -4.2401e-01,\n",
            "         -6.9758e-02,  2.5220e-01,  8.0843e-01, -1.8916e-01, -4.9056e-01,\n",
            "          1.5367e+00,  2.0474e+00,  1.4219e+00,  1.4950e+00, -6.9123e-01,\n",
            "         -3.9482e-01,  2.3657e-01,  7.3383e-01, -5.3185e-01, -9.7398e-01],\n",
            "        [ 7.0228e-01,  2.0456e+00,  6.7268e-01,  5.7795e-01, -8.4048e-01,\n",
            "         -3.8680e-02,  4.6588e-02,  1.0578e-01, -8.0912e-01, -8.9559e-01,\n",
            "          1.8489e-01, -2.5737e-01,  2.7669e-01,  1.8070e-01, -3.7934e-01,\n",
            "          6.6128e-01,  5.1083e-01,  6.1216e-01, -8.9142e-01,  3.6727e-02,\n",
            "          5.6136e-01,  1.3749e+00,  5.7900e-01,  4.2791e-01, -8.0959e-01,\n",
            "          3.5074e-01,  3.2677e-01,  4.1407e-01, -1.1045e+00, -3.1841e-01],\n",
            "        [ 1.8383e+00,  2.3365e+00,  1.9825e+00,  1.7352e+00,  1.5258e+00,\n",
            "          3.2721e+00,  3.2969e+00,  2.6589e+00,  2.1372e+00,  1.0437e+00,\n",
            "          1.1579e+00,  6.8609e-01,  1.4385e+00,  1.0095e+00, -1.7300e-01,\n",
            "          2.0177e+00,  1.3023e+00,  7.8572e-01,  3.2663e-01,  9.0406e-01,\n",
            "          1.9612e+00,  2.2379e+00,  2.3036e+00,  1.6532e+00,  1.4304e+00,\n",
            "          3.9048e+00,  3.1976e+00,  2.2900e+00,  1.9191e+00,  2.2196e+00],\n",
            "        [-1.8084e+00,  1.2218e+00, -1.8144e+00, -1.3478e+00, -3.1121e+00,\n",
            "         -1.1508e+00, -1.1149e+00, -1.2618e+00, -8.2007e-01, -5.6103e-01,\n",
            "         -7.0279e-02,  3.8309e-01, -1.5745e-01, -4.6615e-01,  4.9342e-02,\n",
            "         -1.1635e+00, -1.0575e+00, -1.9134e+00,  7.5283e-01, -3.8275e-01,\n",
            "         -1.4109e+00,  7.6419e-01, -1.4327e+00, -1.0758e+00, -1.8590e+00,\n",
            "         -1.2076e+00, -1.3058e+00, -1.7451e+00, -4.8138e-02, -7.5121e-01]])\n",
            "Target Shape: torch.Size([29])\n",
            "Output Shape: torch.Size([29, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch.distributions as dist\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_samples = 2\n",
        "for i, (inputs, targets) in enumerate(trainloader):\n",
        "  optimizer.zero_grad\n",
        "  print(f'Input Shape: {inputs.shape}')\n",
        "  print(inputs)\n",
        "  print(f'Target Shape: {targets.shape}')\n",
        "  outputs = network(inputs)\n",
        "  print(f'Output Shape: {outputs.shape}')\n",
        "  output_probs = torch.softmax(outputs, dim=-1)\n",
        "  distribution = dist.Categorical(output_probs)\n",
        "  calc_act = True\n",
        "  for j in range(n_samples):\n",
        "    samples = distribution.sample()\n",
        "    loss = loss_fn(outputs, samples)\n",
        "    loss.backward(retain_graph=True)\n",
        "    precond.calculate_cov(calc_act=calc_act)\n",
        "    network.zero_grad()\n",
        "    calc_act = False\n",
        "  #loss = criterion(outputs, targets.reshape(-1,1))\n",
        "  #loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=30, out_features=32, bias=True)\n",
            "Activation Cov Matrix len: 10\n",
            "[0.8272504210472107, 0.22340252995491028, 0.824838399887085, 0.7830191254615784, 0.0704726055264473]\n",
            "[1.0926154851913452, 0.41147610545158386, 1.098839521408081, 1.0061488151550293, 0.026723619550466537]\n",
            "[0.8906545042991638, 0.3817518949508667, 0.8857139348983765, 0.8572977185249329, 0.1744510531425476]\n",
            "[1.4418561458587646, 0.562223494052887, 1.435240387916565, 1.5619181394577026, 0.19939666986465454]\n",
            "Output Cov Matrix len: 20\n",
            "Activation Cov Matrix shape: torch.Size([31, 31])\n",
            "Output Cov Matrix shape: torch.Size([32, 32])\n",
            "Linear(in_features=32, out_features=64, bias=True)\n",
            "Activation Cov Matrix len: 10\n",
            "[0.09568451344966888, 0.014156293123960495, 0.09352162480354309, 0.012460540048778057, 0.059680547565221786]\n",
            "[0.17700529098510742, 0.04611698165535927, 0.13976630568504333, 0.0574762187898159, 0.07323087006807327]\n",
            "[0.18632617592811584, 0.039563439786434174, 0.13833092153072357, 0.024181123822927475, 0.1765521615743637]\n",
            "[0.07058145850896835, 0.023563329130411148, 0.05690966546535492, 0.005692372564226389, 0.07210123538970947]\n",
            "Output Cov Matrix len: 20\n",
            "Activation Cov Matrix shape: torch.Size([33, 33])\n",
            "Output Cov Matrix shape: torch.Size([64, 64])\n",
            "Linear(in_features=64, out_features=1, bias=True)\n",
            "Activation Cov Matrix len: 10\n",
            "[0.018378132954239845, 0.002891946816816926, 0.02438117377460003, 0.002100131008774042, 0.016252262517809868]\n",
            "[0.014273939654231071, 0.002608750481158495, 0.018601106479763985, 0.0029885112307965755, 0.01397517416626215]\n",
            "[0.019208768382668495, 0.006254313513636589, 0.024044787511229515, 0.0019855606369674206, 0.013708183541893959]\n",
            "[0.038016900420188904, 0.0034088832326233387, 0.041364267468452454, 0.004512199200689793, 0.05113009735941887]\n",
            "Output Cov Matrix len: 20\n",
            "Activation Cov Matrix shape: torch.Size([65, 65])\n",
            "Output Cov Matrix shape: torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "for group in precond.param_groups:\n",
        "  print(group['mod'])\n",
        "  print(f'Activation Cov Matrix len: {len(group[\"A\"])}')\n",
        "  print(group['A'][0].tolist()[0][:5])\n",
        "  print(group['A'][1].tolist()[0][:5])\n",
        "  print(group['A'][2].tolist()[0][:5])\n",
        "  print(group['A'][3].tolist()[0][:5])\n",
        "  print(f'Output Cov Matrix len: {len(group[\"S\"])}')\n",
        "  print(f'Activation Cov Matrix shape: {group[\"A\"][0].shape}')\n",
        "  print(f'Output Cov Matrix shape: {group[\"S\"][0].shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.0072457790374756, 0.31007063388824463, 1.0060679912567139, 0.9909204244613647, 0.1981646716594696]\n",
            "Averaged activation matrix: torch.Size([31, 31])\n",
            "Averaged output matrix: torch.Size([32, 32])\n",
            "[0.11650141328573227, 0.031128928065299988, 0.09232742339372635, 0.02067553624510765, 0.08652587234973907]\n",
            "Averaged activation matrix: torch.Size([33, 33])\n",
            "Averaged output matrix: torch.Size([64, 64])\n",
            "[0.02231888845562935, 0.00450670812278986, 0.026402030140161514, 0.0029004632961004972, 0.021502971649169922]\n",
            "Averaged activation matrix: torch.Size([65, 65])\n",
            "Averaged output matrix: torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "for group in precond.param_groups:\n",
        "    A = torch.stack(group['A']).mean(dim=0)\n",
        "    print(A.tolist()[0][:5])\n",
        "    S = torch.stack(group['S']).mean(dim=0)\n",
        "\n",
        "    print(f'Averaged activation matrix: {A.shape}')\n",
        "    print(f'Averaged output matrix: {S.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
