{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexgill321/EKFAC-Influence-Benchmarks/blob/main/EKFAC_calculate_if_value.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfuGSfjLeTY8",
        "outputId": "522e5d66-60b2-43d9-94cc-c719bdaed571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of x: (569, 30)\n",
            "shape of y: (569,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "x = data['data']\n",
        "y = data['target']\n",
        "print(\"shape of x: {}\\nshape of y: {}\".format(x.shape,y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E4K2raONmQY6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "u3Tjibo6eY7H"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32)\n",
        "    self.y = torch.tensor(y,dtype=torch.float32)\n",
        "    self.length = self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "trainset = dataset(X_train,Y_train)\n",
        "trainloader = DataLoader(trainset,batch_size=60,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "valset = dataset(X_test,Y_test)\n",
        "valloader = DataLoader(valset,batch_size=60,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "LZoApzRteh1b"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,input_shape):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_shape,32)\n",
        "    self.fc2 = nn.Linear(32,64)\n",
        "    self.fc3 = nn.Linear(64,1)\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "eBt73zD1goku"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "network = Net(x.shape[1])\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_0rqkldreouB"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "class EKFACDistilled(Optimizer):\n",
        "    def __init__(self, net, eps):\n",
        "        self.eps = eps\n",
        "        self.params = []\n",
        "        self._fwd_handles = []\n",
        "        self._bwd_handles = []\n",
        "        self.net = net\n",
        "        for mod in net.modules():\n",
        "          mod_class = mod.__class__.__name__\n",
        "          if mod_class in ['Linear']:\n",
        "              handle = mod.register_forward_pre_hook(self._save_input)\n",
        "              self._fwd_handles.append(handle)\n",
        "              handle = mod.register_full_backward_hook(self._save_grad_output)\n",
        "              self._bwd_handles.append(handle)\n",
        "              params = [mod.weight]\n",
        "              if mod.bias is not None:\n",
        "                  params.append(mod.bias)\n",
        "              d = {'params': params, 'mod': mod, 'layer_type': mod_class}\n",
        "              self.params.append(d)\n",
        "        super(EKFACDistilled, self).__init__(self.params, {})\n",
        "\n",
        "    def step(self, update_stats=True, update_params=True):\n",
        "        for group in self.param_groups:\n",
        "            if len(group['params']) == 2:\n",
        "                weight, bias = group['params']\n",
        "            else:\n",
        "                weight = group['params'][0]\n",
        "                bias = None\n",
        "            state = self.state[weight]\n",
        "\n",
        "            self._compute_kfe(group, state)\n",
        "\n",
        "            self._precond(weight, bias, group, state)\n",
        "\n",
        "    def _compute_kfe(self, group, state):\n",
        "        mod = group['mod']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        print(f\"Shape of x: {x.shape}\")\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        print(f\"Shape of gy: {gy.shape}\")\n",
        "\n",
        "        # Computation of xxt\n",
        "        x = x.data.t() # transpose of activations\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if mod.bias is not None:\n",
        "            ones = torch.ones_like(x[:1])\n",
        "            x = torch.cat([x, ones], dim=0)\n",
        "\n",
        "        # Calculate covariance matrix for activations (A_{l-1})\n",
        "        xxt = torch.mm(x, x.t()) / float(x.shape[1])\n",
        "\n",
        "        print(f'A cov matrix shape: {xxt.shape}')\n",
        "\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaA, QA)\n",
        "        la, Qa = torch.linalg.eigh(xxt, UPLO='U')\n",
        "        state['Qa'] = Qa\n",
        "        print(f'Qa eigenvec shape: {Qa.shape}')\n",
        "        print(f'LambdaA eigenval vec shape: {la.shape}')\n",
        "        # Computation of ggt\n",
        "        gy = gy.data.t()\n",
        "\n",
        "        # Calculate covariance matrix for layer outputs (S_{l})\n",
        "        ggt = torch.mm(gy, gy.t()) / float(gy.shape[1])\n",
        "\n",
        "        print(f'S cov matrix shape: {ggt.shape}')\n",
        "        # Calculate eigenvalues and eigenvectors of covariance matrix (lambdaS, QS)\n",
        "        ls, Qs = torch.linalg.eigh(ggt, UPLO='U')\n",
        "\n",
        "        G_real = torch.kron(xxt,ggt)\n",
        "        print(f'AxS direct shape: {G_real.shape}')\n",
        "\n",
        "        state['Qs'] = Qs\n",
        "\n",
        "        print(f'Qs eigenvec shape: {Qs.shape}')\n",
        "        print(f'LambdaS eigenval vec shape: {ls.shape}')\n",
        "\n",
        "        prod_as = torch.kron(Qa, Qs)\n",
        "\n",
        "        print(f'Kroneker product of Qa * Qs: {prod_as.shape}')\n",
        "\n",
        "        prod_eigval = torch.kron(torch.diag(la),torch.diag(ls))\n",
        "\n",
        "        G = torch.matmul(prod_as,torch.matmul(prod_eigval, prod_as.t()))\n",
        "\n",
        "        print(f'G SHAPE: {G.shape}')\n",
        "\n",
        "        print(f'Kroneker product of LambdaA * LambdaS: {prod_eigval.shape}')\n",
        "\n",
        "        # Outer product of the eigenvalue vectors. Of shape (len(s) x len(a))\n",
        "        state[\"m2\"] = m2 = ls.unsqueeze(1) * la.unsqueeze(0)\n",
        "        print(f\"eigenval outer product shape: {m2.shape}\")\n",
        "\n",
        "        print(G_real - G)\n",
        "\n",
        "    def _precond(self, weight, bias, group, state):\n",
        "        \"\"\"Applies preconditioning.\"\"\"\n",
        "        Qa = state['Qa']\n",
        "        Qs = state['Qs']\n",
        "        m2 = state['m2']\n",
        "        x = self.state[group['mod']]['x']\n",
        "        print(x)\n",
        "        gy = self.state[group['mod']]['gy']\n",
        "        g = weight.grad.data\n",
        "        s = g.shape\n",
        "        s_x = x.size()\n",
        "        s_gy = gy.size()\n",
        "        bs = x.size(0)\n",
        "\n",
        "        # Append column of ones to x if bias is not None\n",
        "        if bias is not None:\n",
        "            ones = torch.ones_like(x[:,:1])\n",
        "            x = torch.cat([x, ones], dim=1)\n",
        "\n",
        "        # KFE of activations ??\n",
        "        x_kfe = torch.mm(x, Qa)\n",
        "\n",
        "        print(f\"KFE of activations a shape: {x_kfe.shape}\")\n",
        "\n",
        "        # KFE of layer outputs ??\n",
        "        gy_kfe = torch.mm(gy, Qs)\n",
        "\n",
        "        print(f\"KFE of outputs gy shape: {gy_kfe.shape}\")\n",
        "\n",
        "        m2 = torch.mm(gy_kfe.t()**2, x_kfe**2) / bs\n",
        "\n",
        "        print(f'kfe squared matrix idk shape: {m2.shape}')\n",
        "        g_kfe = torch.mm(gy_kfe.t(), x_kfe) / bs\n",
        "\n",
        "        print(f'g_kfe shape: {g_kfe.shape}')\n",
        "\n",
        "        g_nat_kfe = g_kfe / (m2 + self.eps)\n",
        "\n",
        "        print(f'g_nat_kfe shape: {g_nat_kfe.shape}')\n",
        "\n",
        "        g_nat = torch.mm(g_nat_kfe, Qs.t())\n",
        "\n",
        "        if bias is not None:\n",
        "            gb = g_nat[:, -1].contiguous().view(*bias.shape)\n",
        "            bias.grad.data = gb\n",
        "            g_nat = g_nat[:, :-1]\n",
        "\n",
        "        g_nat = g_nat.contiguous().view(*s)\n",
        "        weight.grad.data = g_nat\n",
        "\n",
        "    def _save_input(self, mod, i):\n",
        "        \"\"\"Saves input of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['x'] = i[0]\n",
        "\n",
        "    def _save_grad_output(self, mod, grad_input, grad_output):\n",
        "        \"\"\"Saves grad on output of layer to compute covariance.\"\"\"\n",
        "        self.state[mod]['gy'] = grad_output[0] * grad_output[0].size(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkJ--HaKgekv",
        "outputId": "a87ce5f3-8d44-45cf-dc72-04585dea7e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n",
            "Linear\n",
            "**********************\n"
          ]
        }
      ],
      "source": [
        "precond = EKFACDistilled(network, eps=0.001)\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "for mod in network.modules():\n",
        "  mod_class = mod.__class__.__name__\n",
        "  print(mod_class)\n",
        "  print(\"**********************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkTEE9XJkIVr"
      },
      "source": [
        "# **Create a score matrix from the query (test datapoint)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nVbKbB3lpTl",
        "outputId": "8a1cad15-0637-41e7-d0b6-252104314dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the linear layer weight grad wrt the output: torch.Size([32, 30])\n",
            "the linear layer bias grad wrt the output: torch.Size([32])\n",
            "torch.Size([992])\n"
          ]
        }
      ],
      "source": [
        "# just need one exmanple from the test set as a \"QUERY\"\n",
        "\n",
        "\n",
        "query_influence_tensor = ''\n",
        "\n",
        "for i, (inputs, targets) in enumerate(valloader):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = network(inputs)\n",
        "  # print(outputs[0].backward())\n",
        "  outputs[0].backward()\n",
        "\n",
        "  grad_wrt_logits_weights = network.fc1.weight.grad\n",
        "  grad_wrt_logits_bias = network.fc1.bias.grad\n",
        "\n",
        "  print(\"the linear layer weight grad wrt the output: {}\".format(grad_wrt_logits_weights.shape)) #32, 30\n",
        "  print(\"the linear layer bias grad wrt the output: {}\".format(grad_wrt_logits_bias.shape)) #32\n",
        "\n",
        "  grad_wrt_logits_bias = grad_wrt_logits_bias.view(32, 1)\n",
        "\n",
        "  total_linear_layer_params = torch.cat((grad_wrt_logits_weights, grad_wrt_logits_bias), dim=1)\n",
        "\n",
        "  query_influence_tensor = total_linear_layer_params.view(-1)\n",
        "  print(query_influence_tensor.shape)\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb9KQxIYDVAs"
      },
      "source": [
        "# **Create the score matrix for each training example (or a random subset of examples), this will be mutlitpled with `query_influence_tensor` and `G`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-dTGiBuDsTV",
        "outputId": "f3a6312b-e1a2-4483-8c2b-771461fe7095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total examples from training set are: 8\n"
          ]
        }
      ],
      "source": [
        "training_example_influence_tensors = {}\n",
        "\n",
        "\n",
        "\n",
        "# we will take the first example from each batch.\n",
        "\n",
        "for i, (inputs, targets) in enumerate(trainloader):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = network(inputs)\n",
        "  probabilities = torch.sigmoid(outputs)\n",
        "\n",
        "  loss = criterion(probabilities, targets.view(-1,1))\n",
        "  loss.backward()\n",
        "\n",
        "\n",
        "  layer_grad_wrt_loss_weights = network.fc1.weight.grad\n",
        "  layer_grad_wrt_loss_bias = network.fc1.bias.grad\n",
        "\n",
        "  # print(\"the linear layer weight grad wrt the loss: {}\".format(layer_grad_wrt_loss_weights.shape)) #32,30\n",
        "  # print(\"the linear layer bias grad wrt the loss: {}\".format(layer_grad_wrt_loss_bias.shape)) #32\n",
        "\n",
        "  layer_grad_wrt_loss_bias = layer_grad_wrt_loss_bias.view(32, 1)\n",
        "\n",
        "\n",
        "  total_linear_layer_params = torch.cat((layer_grad_wrt_loss_weights, layer_grad_wrt_loss_bias), dim=1)\n",
        "  training_influence_tensor = total_linear_layer_params.view(992)\n",
        "\n",
        "  training_example_influence_tensors[i] = training_influence_tensor\n",
        "\n",
        "print('total examples from training set are: {}'.format(len(training_example_influence_tensors)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBZIyQ6GJ3TD"
      },
      "source": [
        "# **Now calculate IF for each example (8 total, one from each batch)**/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kek5YBJQJ9Hc",
        "outputId": "686da16d-9a66-4fe4-af0c-98b039204b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0296])\n",
            "tensor([0.0775])\n",
            "tensor([0.1017])\n",
            "tensor([0.1043])\n",
            "tensor([0.0847])\n",
            "tensor([0.0820])\n",
            "tensor([0.0787])\n",
            "tensor([0.1452])\n",
            "highest training influence example is example 0 form batch 7 with value 0.1451510787010193\n"
          ]
        }
      ],
      "source": [
        "# taking a temp gaussian approximation vector\n",
        "approx_gaussian = torch.randn(992, 992)\n",
        "\n",
        "\n",
        "influence_values = []\n",
        "\n",
        "for key, value in training_example_influence_tensors.items():\n",
        "  val = torch.matmul(value,torch.matmul(approx_gaussian, query_influence_tensor).view(992, 1))\n",
        "  print(val)\n",
        "  influence_values.append(val.item())\n",
        "\n",
        "\n",
        "print(\"highest training influence example is example 0 form batch {} with value {}\".format(influence_values.index(max(influence_values)), max(influence_values)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOBA0MZ2KgmF2ksdEaIBX5+",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.17 ('img_proc_proj0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "4f16748fa5b9c7c97742513131751f6102eff5e936588c180718494a87f90c77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
